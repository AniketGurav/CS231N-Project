{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovingBox Intermediate Frame Prediction by LSTM | Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "from util.parser import *\n",
    "from util.img_kit import *\n",
    "from util.notebook_display import *\n",
    "from util.numeric_ops import *\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "from util.tf_ops import *\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "from os import walk\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0) # set default size of plots\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': (56, 64, 64), 'images:': 'bigsquare-vertical-4'}\n",
      "{'dim': (56, 64, 64), 'images:': 'bigCircle-3'}\n",
      "{'dim': (56, 64, 64), 'images:': 'circle-diagnal-2'}\n",
      "{'dim': (56, 64, 64), 'images:': 'bigSquare-diagnal-2'}\n",
      "{'dim': (56, 64, 64), 'images:': 'diamond-vertical1'}\n",
      "{'dim': (56, 64, 64), 'images:': 'bigSquare-vertical-2'}\n",
      "{'dim': (56, 64, 64), 'images:': 'diamond-vertical2'}\n",
      "{'dim': (56, 64, 64), 'images:': 'bigCircle-2'}\n",
      "{'dim': (56, 64, 64), 'images:': 'diamond-vertical3'}\n",
      "{'dim': (56, 64, 64), 'images:': 'diamond-diagnal1'}\n",
      "{'dim': (56, 64, 64), 'images:': 'bigsquare-vertical-5'}\n",
      "{'dim': (56, 64, 64), 'images:': 'circle'}\n",
      "{'dim': (56, 64, 64), 'images:': 'circle-diagnal'}\n",
      "{'dim': (56, 64, 64), 'images:': 'diamond-diagnal2'}\n",
      "{'dim': (56, 64, 64), 'images:': 'bigCircle-4'}\n",
      "{'dim': (56, 64, 64), 'images:': 'bigSquare-vertical-3'}\n",
      "{'dim': (56, 64, 64), 'images:': 'bigSquare-vertical'}\n",
      "\n",
      "After Augmentation: img_collections has 68 collections, 3808 images in total\n"
     ]
    }
   ],
   "source": [
    "train_collection =  get_collection(\"data/moving-box/64x64/train\")\n",
    "train_collection = augment_reverse_color(train_collection)\n",
    "train_collection = augment_reverse_sequence(train_collection)\n",
    "train_collection = center_collections(train_collection)\n",
    "\n",
    "# total number of images\n",
    "total_train = sum([x.shape[0] for x in train_collection])\n",
    "print(\"\\nAfter Augmentation: img_collections has {} collections, {} images in total\".format(len(train_collection), total_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': (56, 64, 64), 'images:': 'bigCircle'}\n",
      "{'dim': (56, 64, 64), 'images:': 'bigSquare-diagnal'}\n",
      "{'dim': (56, 64, 64), 'images:': 'diamond'}\n",
      "\n",
      "After Augmentation: Test set has 6 collections, 336 images in total\n"
     ]
    }
   ],
   "source": [
    "test_collection = get_collection(\"data/moving-box/64x64/test\")\n",
    "test_collection = augment_reverse_color(test_collection)\n",
    "test_collection = center_collections(test_collection)\n",
    "# total number of images\n",
    "total_test = sum([x.shape[0] for x in test_collection])\n",
    "print(\"\\nAfter Augmentation: Test set has {} collections, {} images in total\".format(len(test_collection), total_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(collection, batch_size = 8, gap = 1, seq_size = 3):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        collection: [img_data] - list of ndarray\n",
    "    Output:\n",
    "        (train_input, train_gd)\n",
    "        \n",
    "        train_input: [batch size, seq_size, 32, 32]\n",
    "        train_gd:    [batch size, seq_size, 32, 32]\n",
    "    \"\"\"\n",
    "    assert gap%2==1, \"Gap must be odd !\" \n",
    "    \n",
    "    def expand_start_to_seq(start_ind):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            start_ind: a number indicating index of start frame\n",
    "        Output:\n",
    "            np array of [start_ind, start_ind + gap +1, start_ind + 2*(gap+1) ...]\n",
    "        \"\"\"\n",
    "        return np.array([start_ind + i * (gap + 1) for i in range(seq_size)])\n",
    "    \n",
    "    \n",
    "    np.random.shuffle(collection)\n",
    "    # get average number of training for each class\n",
    "    n_collection = len(collection)\n",
    "    num_per_collection = [x.shape[0] for x in collection]\n",
    "    avg_num_per_class = int(np.ceil(batch_size/n_collection))\n",
    "    # start-index for each class\n",
    "    start_ind = []\n",
    "    for i, imgs in enumerate(collection):\n",
    "        try:\n",
    "            s = np.random.choice(range(num_per_collection[i] - (gap + 1) * seq_size), avg_num_per_class, replace=False)\n",
    "            start_ind.append(s)\n",
    "        except: # if not enough in this class\n",
    "            print(\"err\")\n",
    "            start_ind.append(np.array([]))\n",
    "    selected_classes = [i for i in range(n_collection) if start_ind[i].shape[0]>0]\n",
    "    train_ind = [[expand_start_to_seq(s) for s in ind] for ind in start_ind] # train indexes for each class\n",
    "    gd_ind = [[(x + (gap+1)//2) for x in ind_by_class] for ind_by_class in train_ind]\n",
    "    train_input = np.concatenate([np.stack([collection[i][j] for j in train_ind[i]]) for i in selected_classes], axis = 0)\n",
    "    train_gd =  np.concatenate([np.stack([collection[i][j] for j in gd_ind[i]]) for i in selected_classes], axis = 0)\n",
    "    \n",
    "    train_input, train_gd = train_input[:batch_size], train_gd[:batch_size]\n",
    "    return train_input, train_gd\n",
    "\n",
    "\n",
    "def sample_train(batch_size = 8, gap = 1, seq_size = 3): return sample(train_collection, batch_size, gap = gap, seq_size = seq_size)\n",
    "\n",
    "def sample_test(batch_size = 8, gap = 1, seq_size = 3):  return sample(test_collection, batch_size, gap, seq_size = seq_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of Image Piece Value: [-1.0, 1.0]\n",
      "seq_input shape:            (4, 6, 64, 64)\n",
      "seq_gd    shape:            (4, 6, 64, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABD4AAACNCAYAAABfRrodAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noZLAoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f756f807940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABD4AAACNCAYAAABfRrodAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f756c8a1668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_sample_train(batch_size, gap, seq_size):\n",
    "    seq_input, seq_gd = sample_train(batch_size=batch_size, gap=gap, seq_size= seq_size)\n",
    "    print(\"Range of Image Piece Value: [{}, {}]\".format(np.min(seq_input), np.max(seq_input)))\n",
    "    print(\"seq_input shape:            {}\".format(seq_input.shape))\n",
    "    print(\"seq_gd    shape:            {}\".format(seq_gd.shape))\n",
    "    selected = np.random.choice(range(batch_size))\n",
    "    size = (20, 2)\n",
    "    plot_images_ndarray(seq_input[selected], title=\"Input Sequence\", size = size)\n",
    "    plot_images_ndarray(seq_gd[selected], title=\"Next Frames\", size = size)\n",
    "    \n",
    "show_sample_train(batch_size = 4, gap = 7, seq_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of Image Piece Value: [-1.0, 1.0]\n",
      "seq_input shape:            (2, 5, 64, 64)\n",
      "seq_gd    shape:            (2, 5, 64, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABB0AAACNCAYAAAD7LALOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDpIkSZIkKQuLDpIkSZIkKYv/ByVyPvkbOUBKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f756f75bd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABB0AAACNCAYAAAD7LALOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAKCJ/w/BowwUrnTa8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f756f8c1b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_sample_test(batch_size, gap, seq_size):\n",
    "    seq_input, seq_gd = sample_test(batch_size=batch_size, gap=gap, seq_size= seq_size)\n",
    "    print(\"Range of Image Piece Value: [{}, {}]\".format(np.min(seq_input), np.max(seq_input)))\n",
    "    print(\"seq_input shape:            {}\".format(seq_input.shape))\n",
    "    print(\"seq_gd    shape:            {}\".format(seq_gd.shape))\n",
    "    selected = np.random.choice(range(batch_size))\n",
    "    size = (20, 2)\n",
    "    plot_images_ndarray(seq_input[selected], title=\"Input Sequence\", size = size)\n",
    "    plot_images_ndarray(seq_gd[selected], title=\"Next Frames\", size = size)\n",
    "    \n",
    "show_sample_test(batch_size = 2, gap = 5, seq_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_size         = 9\n",
    "feature_size     = 1024*8    # size of feature vector for LSTM\n",
    "lstm_state_size  = feature_size   # size of hidden state: [lstm_state_size, lstm_state_size]\n",
    "\n",
    "num_iteration    = 4000\n",
    "gap              = 1\n",
    "batch_size       = 8\n",
    "learning_rate    = 1.6e-4\n",
    "beta             = 0.9\n",
    "\n",
    "assert feature_size%64 == 0, \"feature_size must be divisable by 64!\"\n",
    "feature_channels = int(feature_size/8/8)\n",
    "\n",
    "model_save_path = \"trained_model/LSTM_box/1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_img(img, is_training=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        batch size of img\n",
    "    Output:\n",
    "        batch size of feature [batch_size, feature_size]\n",
    "    \"\"\"\n",
    "    x = img\n",
    "    x = tf.reshape(img, [-1, 64, 64, 1])\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters = 64, kernel_size=4, strides=2, padding='same', activation=tf.nn.relu)\n",
    "\n",
    "    x = tf.layers.conv2d(x, filters = 64, kernel_size=3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "\n",
    "    x = tf.layers.conv2d(x, filters = 64, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters = feature_channels, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "    x = tf.contrib.layers.flatten(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_seq(img_seq, seq_size = seq_size):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        img_seq: sequence of images      Tensor         [batch_size, seq_size, 32, 32]\n",
    "    Output:\n",
    "        encoded feature of the sequence  List of Tensor [batch_size, feature_size] of length seq_size\n",
    "    \"\"\"\n",
    "    img_seq = tf.transpose(img_seq, perm=[1, 0, 2, 3]) # [seq_size, batch_size, 32, 32, 1]\n",
    "    \n",
    "    return [encode_img(img_seq[i]) for i in range(seq_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(feature, is_training=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        batch size of feature [batch_size, 8, 8, feature_channel]\n",
    "    Output:\n",
    "        batch size of img [batch_size, 32, 32, 1]\n",
    "    \"\"\"\n",
    "    x = tf.reshape(feature, [-1, 16, 16, feature_channels])\n",
    "    x = tf.layers.conv2d_transpose(x, filters=32, kernel_size=2,  strides=1, activation=tf.nn.tanh, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=64, kernel_size=4, strides=2, activation=tf.nn.relu, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=32, kernel_size=3,  strides=2, activation=tf.nn.tanh, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=16, kernel_size=2,  strides=1, activation=tf.nn.tanh, padding='same')\n",
    "    img = tf.layers.conv2d_transpose(x, filters=1, kernel_size=2,  strides=1, activation=tf.nn.tanh, padding='same')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(gd_imgs, output_imgs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        gd_imgs, output_imgs: [batch_size, seq_size, 8, 8, 1]\n",
    "    Output:\n",
    "        scaler loss\n",
    "    \"\"\"\n",
    "    gd_imgs, output_imgs = tf.contrib.layers.flatten(gd_imgs), tf.contrib.layers.flatten(output_imgs)\n",
    "    return tf.norm(gd_imgs - output_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_loss(loss, seq_size = seq_size, batch_size = batch_size):\n",
    "    return loss/seq_size/batch_size/2*255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_solver(learning_rate=1e-3, beta1=0.5):\n",
    "    return tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batch_seq        = tf.placeholder(tf.float32, [None, seq_size, 64, 64], name = \"batch_seq\")\n",
    "batch_next       = tf.placeholder(tf.float32, [None, seq_size, 64, 64], name = \"batch_next\")\n",
    "is_training      = tf.placeholder(tf.bool, (), name = \"is_training\")\n",
    "\n",
    "feature_seq      = encode_seq(batch_seq)\n",
    "\n",
    "lstm_cell1       = rnn.BasicLSTMCell(lstm_state_size)\n",
    "# lstm_cell2       = rnn.BasicLSTMCell(lstm_state_size)\n",
    "# Cell             = rnn.MultiRNNCell([lstm_cell1, lstm_cell2])\n",
    "Cell = lstm_cell1\n",
    "output_feature, states = rnn.static_rnn(Cell, feature_seq, dtype=tf.float32)\n",
    "\n",
    "output_imgs = tf.stack([decode(f) for f in output_feature], axis = 1)  # [seq_size, batch_size, 32, 32, 1]\n",
    "\n",
    "loss = get_loss(batch_next, output_imgs)\n",
    "\n",
    "\n",
    "solver = get_solver(learning_rate, beta)\n",
    "\n",
    "train_step = solver.minimize(loss)\n",
    "\n",
    "# add to saver\n",
    "tf.add_to_collection('output_batch_img', output_imgs)\n",
    "tf.add_to_collection('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, train_step, loss, batch_size, num_iteration, \\\n",
    "          plot_every = 400, show_loss_every=400, num_plot = 6,  save_every = 1000):\n",
    "    losses = []\n",
    "    saver = tf.train.Saver()\n",
    "    for i in range(1, num_iteration+1):\n",
    "        # get a sample\n",
    "        # gap = np.random.choice([1,3,5,7,9])\n",
    "        seq_input, seq_gd = sample_train(batch_size, gap, seq_size = seq_size)\n",
    "        dic = {batch_seq: seq_input, batch_next: seq_gd, is_training: True}\n",
    "        \n",
    "        sess.run([train_step], dic)\n",
    "        curr_loss = sess.run(loss, dic)\n",
    "        curr_loss = scale_loss(curr_loss)# tweek loss to match report loss\n",
    "        \n",
    "        losses.append(curr_loss)\n",
    "    \n",
    "        if i%show_loss_every ==0:\n",
    "            print(\"Iteration {}:  loss = {} | Gap = {}\".format(i, curr_loss, gap))\n",
    "            \n",
    "        if i%plot_every == 0:\n",
    "            seq_input, seq_gd = sample_train(1, gap, seq_size = seq_size)\n",
    "            seq_generated = sess.run(output_imgs, feed_dict=\\\n",
    "                                      {batch_seq: seq_input, batch_next: seq_gd, is_training: False})\n",
    "            seq_generated = seq_generated[0]\n",
    "            plot_batch_images(seq_generated[:num_plot], (16, 2) , \"Iteration: {} | gap = {}\".format(i + plot_every, gap))\n",
    "        if i%save_every == 0:\n",
    "            saver.save(sess, model_save_path, global_step = i)   \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Dst tensor is not initialized.\n\t [[Node: zeros_144 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [40960,32768] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'zeros_144', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 405, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-49-30f69753d1f8>\", line 22, in <module>\n    train_step = solver.minimize(loss)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 289, in minimize\n    name=name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 403, in apply_gradients\n    self._create_slots(var_list)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/adam.py\", line 117, in _create_slots\n    self._zeros_slot(v, \"m\", self._name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 647, in _zeros_slot\n    named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/slot_creator.py\", line 121, in create_zeros_slot\n    val = array_ops.zeros(primary.get_shape().as_list(), dtype=dtype)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1352, in zeros\n    output = constant(zero, shape=shape, dtype=dtype, name=name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 103, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Dst tensor is not initialized.\n\t [[Node: zeros_144 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [40960,32768] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[Node: zeros_144 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [40960,32768] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-3b4db140b41f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m,\u001b[0m                \u001b[0mplot_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_loss_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_plot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0msave_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1035\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[Node: zeros_144 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [40960,32768] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'zeros_144', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 405, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-49-30f69753d1f8>\", line 22, in <module>\n    train_step = solver.minimize(loss)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 289, in minimize\n    name=name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 403, in apply_gradients\n    self._create_slots(var_list)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/adam.py\", line 117, in _create_slots\n    self._zeros_slot(v, \"m\", self._name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 647, in _zeros_slot\n    named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/slot_creator.py\", line 121, in create_zeros_slot\n    val = array_ops.zeros(primary.get_shape().as_list(), dtype=dtype)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1352, in zeros\n    output = constant(zero, shape=shape, dtype=dtype, name=name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 103, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Dst tensor is not initialized.\n\t [[Node: zeros_144 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [40960,32768] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "sess = get_session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "losses = train(sess, train_step, loss, batch_size, num_iteration, \\\n",
    "               plot_every = 40, show_loss_every = 40, num_plot=7,  save_every = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figsize = (20, 8)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Generator Losses\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.plot(losses[-100:])\n",
    "plt.title(\"Generator Losses - Last 1000\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_loss(name, num_run = 100, gap = 3, batch_size = batch_size, seq_size = 3):\n",
    "    losses = []\n",
    "    for _ in range(num_run):\n",
    "        if name == \"train\": seq_input, seq_gd = sample_train(1, gap, seq_size = seq_size)\n",
    "        else:               seq_input, seq_gd = sample_test(1, gap, seq_size = seq_size)\n",
    "        dic = {batch_seq: seq_input, batch_next: seq_gd, is_training: True}\n",
    "        curr_loss = sess.run(loss, dic)\n",
    "        losses.append(curr_loss)\n",
    "    return scale_loss(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_generattion(seq_input):\n",
    "    feed_dict={batch_seq: seq_input, is_training: False}\n",
    "    gen_batch = sess.run(output_imgs, feed_dict)\n",
    "    return gen_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_generations(name, seq_size = 6, gap = 3):\n",
    "    if name == \"train\":  seq_input, seq_gd = sample_train(1, gap, seq_size = seq_size)\n",
    "    else:                seq_input, seq_gd = sample_test(1, gap, seq_size = seq_size)\n",
    "    \n",
    "    seq_generated = get_generattion(seq_input)\n",
    "    seq_generated, seq_input, seq_gd = seq_generated[0], seq_input[0], seq_gd[0]\n",
    "    size = (20, 2)\n",
    "    plot_images_ndarray(seq_input, title=\"Input Sequence\", size = size)\n",
    "    plot_images_ndarray(seq_gd, title=\"Next Frames - Ground Truth\", size = size)\n",
    "    plot_images_ndarray(seq_generated, title=\"Next Frames - Generated\", size = size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Evaluate on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_train(seq_size = 3, gap = 3):\n",
    "    show_generations(\"train\", seq_size, gap)\n",
    "    loss = report_loss(\"train\", 100, gap, batch_size, seq_size = seq_size)\n",
    "    print(\"Training Loss = {}\".format(loss))\n",
    "    return loss\n",
    "\n",
    "train_loss = eval_train(seq_size = seq_size, gap = gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_test(seq_size = 3, gap = 3):\n",
    "    show_generations(\"test\", seq_size, gap)\n",
    "    loss = report_loss(\"test\", 100, gap, batch_size, seq_size = seq_size)\n",
    "    print(\"Test Loss = {}\".format(loss))\n",
    "    return loss\n",
    "\n",
    "test_loss = eval_test(seq_size = seq_size, gap = gap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
