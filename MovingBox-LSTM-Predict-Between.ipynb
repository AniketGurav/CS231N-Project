{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovingBox Intermediate Frame Prediction by LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "from util.parser import *\n",
    "from util.img_kit import *\n",
    "from util.notebook_display import *\n",
    "from util.numeric_ops import *\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "from os import walk\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0) # set default size of plots\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(collection, batch_size = 8, gap = 1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        collection: [img_data] - list of ndarray\n",
    "    Output:\n",
    "        (train_input, train_td)\n",
    "        \n",
    "        train_input: [batch size, seq_size,   32, 32]\n",
    "        train_td:    [batch size, seq_size, 32, 32]\n",
    "    \"\"\"\n",
    "    assert gap%2==1, \"Gap must be odd !\"      \n",
    "    np.random.shuffle(collection)\n",
    "    # get average number of training for each class\n",
    "    n_collection = len(collection)\n",
    "    num_per_collection = [x.shape[0] for x in collection]\n",
    "    avg_num_per_class = int(np.ceil(batch_size/n_collection))\n",
    "    # start-index for each class\n",
    "    start_ind = []\n",
    "    for i, imgs in enumerate(collection):\n",
    "        try:\n",
    "            s = np.random.choice(range(num_per_collection[i] - (gap + 1) * seq_size), avg_num_per_class, replace=False)\n",
    "            start_ind.append(s)\n",
    "        except: # if not enough in this class\n",
    "            start_ind.append(np.array([]))\n",
    "    # end-index for each class\n",
    "    end_ind = [x+gap+1 for x in before_ind]\n",
    "    # mid-index for each class\n",
    "    mid_ind = [x+(gap+1)//2 for x in before_ind]\n",
    "    \n",
    "    selected_classes = [i for i in range(n_collection) if before_ind[i].shape[0]>0]\n",
    "    before_imgs = np.concatenate([collection[i][before_ind[i]] for i in selected_classes], axis = 0)\n",
    "    after_imgs = np.concatenate([collection[i][after_ind[i]] for i in selected_classes], axis = 0)\n",
    "    mid_imgs = np.concatenate([collection[i][mid_ind[i]] for i in selected_classes], axis = 0)\n",
    "    \n",
    "    before_imgs = before_imgs[:batch_size]\n",
    "    mid_imgs = mid_imgs[:batch_size]\n",
    "    after_imgs = after_imgs[:batch_size]\n",
    "    return before_imgs, after_imgs, mid_imgs\n",
    "\n",
    "\n",
    "def sample_train(batch_size = 8, gap = 1): return sample(train_collection, batch_size, gap = gap)\n",
    "\n",
    "def sample_test(batch_size = 8, gap = 1):  return sample(test_collection, batch_size, gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_size        = 3\n",
    "feature_size    = 1024    # size of feature vector for LSTM\n",
    "lstm_state_size = 512   # size of hidden state: [lstm_state_size, lstm_state_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_img(img, is_training=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        batch size of img\n",
    "    Output:\n",
    "        batch size of feature [batch_size, 20, 32, feature_channel]\n",
    "    \"\"\"\n",
    "    x = img\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters = 48, kernel_size=8, strides=2, padding='same', activation=tf.nn.relu)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters = 48, kernel_size=5, strides=2, padding='same', activation=tf.nn.relu)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters = 48, kernel_size=4, strides=2, padding='same', activation=tf.nn.relu)\n",
    "\n",
    "    x = tf.layers.conv2d(x, filters = 48, kernel_size=3, strides=1, padding='same', activation=tf.nn.relu)\n",
    "\n",
    "    x = tf.layers.conv2d(x, filters = 48, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters = feature_channel, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_seq(img_seq):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        img_seq: sequence of images      [ [batch_size, 32, 32, 1] x size seq_size ]\n",
    "    Output:\n",
    "        encoded feature of the sequence  [batch_size, feature_size]\n",
    "    \"\"\"\n",
    "    \n",
    "    return [encode_img(img) for img in img_seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(feature, is_training=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        batch size of feature [batch_size, 32, 32, feature_channel]\n",
    "    Output:\n",
    "        batch size of img [batch_size, 160, 256, 3]\n",
    "    \"\"\"\n",
    "    x = feature\n",
    "\n",
    "    x = tf.layers.conv2d(x, filters = 84, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.conv2d_transpose(x, filters=48, kernel_size=5, strides=2, activation=tf.nn.relu, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=48, kernel_size=7, strides=2, activation=tf.nn.relu, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=48, kernel_size=4, strides=2, activation=tf.nn.relu, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=48, kernel_size=3,  strides=1, activation=tf.nn.tanh, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=24, kernel_size=2,  strides=1, activation=tf.nn.tanh, padding='same')\n",
    "    img = tf.layers.conv2d_transpose(x, filters=3, kernel_size=2,  strides=1, activation=tf.nn.tanh, padding='same')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_in_between(seq_feature, frame1, frame2):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        seq_feature:     encoded feature of image sequence  [batch_size, 8,  8,  feature_dim]\n",
    "        frame1, frame2:  two consecutive frames             [batch_size,32, 32,  1]  \n",
    "    Output:\n",
    "        frame in between\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(gd_imgs, output_imgs):\n",
    "    return tf.norm(x-gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_seq        = tf.placeholder(tf.float32, [None, seq_size, 32, 32, 1])\n",
    "is_training      = tf.placeholder(tf.bool, ())\n",
    "\n",
    "\n",
    "feature_seq      =  encode_seq(batch_seq)\n",
    "lstm_cell = rnn.BasicLSTMCell(lstm_state_size)\n",
    "splitted_fea_seq = tf.split(feature_seq, seq_size, axis=1) # [None, 32, 32, 1]\n",
    "output_feature, states = rnn.static_rnn(lstm_cell, splitted_fea_seq)\n",
    "output_imgs = np.array([decode(f) for f in output_feature])  # [batch_size, 32, 32, 1]\n",
    "\n",
    "loss = get_loss()\n",
    "# alpha   = tf.Variable(0., \"alpha\") # parameter for leaky relu\n",
    "G_batch = generate(batch_before, 1, batch_after)\n",
    "\n",
    "\n",
    "G_loss = content_loss(G_batch, batch_mid)\n",
    "G_solver = get_solver(learning_rate, beta)\n",
    "\n",
    "G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'generator') \n",
    "\n",
    "G_train_step = G_solver.minimize(G_loss, var_list=G_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
