{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovingBox Intermediate Frame Prediction by LSTM | Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from util import *\n",
    "from util.parser import *\n",
    "from util.img_kit import *\n",
    "from util.notebook_display import *\n",
    "from util.numeric_ops import *\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "from util.tf_ops import *\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "from os import walk\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0) # set default size of plots\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images:': 'triangle', 'dim': (50, 32, 32)}\n",
      "{'images:': 'rectangle', 'dim': (56, 32, 32)}\n",
      "{'images:': 'trangle-vertical', 'dim': (56, 32, 32)}\n",
      "{'images:': 'circle-diagnal', 'dim': (56, 32, 32)}\n",
      "{'images:': 'bigsquare-vertical-4', 'dim': (56, 32, 32)}\n",
      "{'images:': 'diamond', 'dim': (56, 32, 32)}\n",
      "{'images:': 'big-diamond-vertical', 'dim': (56, 32, 32)}\n",
      "{'images:': 'diamond-vertical3', 'dim': (56, 32, 32)}\n",
      "{'images:': 'square-diagnal-2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'square-vertical-4', 'dim': (56, 32, 32)}\n",
      "{'images:': 'big-diamond-diagnal1', 'dim': (56, 32, 32)}\n",
      "{'images:': 'big-diamond-vertical2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'rectangle', 'dim': (56, 32, 32)}\n",
      "{'images:': 'big-diamond-vertical3', 'dim': (56, 32, 32)}\n",
      "{'images:': 'bigSquare-vertical', 'dim': (56, 32, 32)}\n",
      "{'images:': 'bigSquare-diagnal-2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'square-vertical-2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'square-vertical-5', 'dim': (56, 32, 32)}\n",
      "{'images:': 'trangle-vertical-3', 'dim': (56, 32, 32)}\n",
      "{'images:': 'trangle-horizontal', 'dim': (56, 32, 32)}\n",
      "{'images:': 'square-vertical-3', 'dim': (56, 32, 32)}\n",
      "{'images:': 'diamond-diagnal1', 'dim': (56, 32, 32)}\n",
      "{'images:': 'bigSquare-vertical-3', 'dim': (56, 32, 32)}\n",
      "{'images:': 'big-diamond-diagnal2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'bigSquare-diagnal', 'dim': (56, 32, 32)}\n",
      "{'images:': 'trangle-vertical-2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'rectangle', 'dim': (56, 32, 32)}\n",
      "{'images:': 'bigsquare-vertical-5', 'dim': (56, 32, 32)}\n",
      "{'images:': 'diamond-vertical1', 'dim': (56, 32, 32)}\n",
      "{'images:': 'diamond-vertical2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'circle', 'dim': (56, 32, 32)}\n",
      "{'images:': 'rectangle', 'dim': (56, 32, 32)}\n",
      "{'images:': 'rectangle', 'dim': (56, 32, 32)}\n",
      "{'images:': 'rectangle', 'dim': (56, 32, 32)}\n",
      "{'images:': 'circle-diagnal-2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'square-vertical', 'dim': (56, 32, 32)}\n",
      "{'images:': 'square-diagnal', 'dim': (56, 32, 32)}\n",
      "{'images:': 'diamond-diagnal2', 'dim': (56, 32, 32)}\n",
      "\n",
      "After Augmentation: img_collections has 152 collections, 8488 images in total\n"
     ]
    }
   ],
   "source": [
    "train_collection =  get_collection(\"data/moving-box/32x32/train\")\n",
    "train_collection = augment_reverse_color(train_collection)\n",
    "train_collection = augment_reverse_sequence(train_collection)\n",
    "train_collection = center_collections(train_collection)\n",
    "\n",
    "# total number of images\n",
    "total_train = sum([x.shape[0] for x in train_collection])\n",
    "print(\"\\nAfter Augmentation: img_collections has {} collections, {} images in total\".format(len(train_collection), total_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images:': 'bigSquare-vertical-2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'big-diamond', 'dim': (56, 32, 32)}\n",
      "{'images:': 'rectangle', 'dim': (56, 32, 32)}\n",
      "\n",
      "After Augmentation: Test set has 6 collections, 336 images in total\n"
     ]
    }
   ],
   "source": [
    "test_collection = get_collection(\"data/moving-box/32x32/test\")\n",
    "test_collection = augment_reverse_color(test_collection)\n",
    "test_collection = center_collections(test_collection)\n",
    "# total number of images\n",
    "total_test = sum([x.shape[0] for x in test_collection])\n",
    "print(\"\\nAfter Augmentation: Test set has {} collections, {} images in total\".format(len(test_collection), total_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(collection, batch_size = 8, gap = 1, seq_size = 3):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        collection: [img_data] - list of ndarray\n",
    "    Output:\n",
    "        (train_input, train_gd)\n",
    "        \n",
    "        train_input: [batch size, seq_size, 32, 32]\n",
    "        train_gd:    [batch size, seq_size, 32, 32]\n",
    "    \"\"\"\n",
    "    assert gap%2==1, \"Gap must be odd !\" \n",
    "    \n",
    "    def expand_start_to_seq(start_ind):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            start_ind: a number indicating index of start frame\n",
    "        Output:\n",
    "            np array of [start_ind, start_ind + gap +1, start_ind + 2*(gap+1) ...]\n",
    "        \"\"\"\n",
    "        return np.array([start_ind + i * (gap + 1) for i in range(seq_size)])\n",
    "    \n",
    "    \n",
    "    np.random.shuffle(collection)\n",
    "    # get average number of training for each class\n",
    "    n_collection = len(collection)\n",
    "    num_per_collection = [x.shape[0] for x in collection]\n",
    "    avg_num_per_class = int(np.ceil(batch_size/n_collection))\n",
    "    # start-index for each class\n",
    "    start_ind = []\n",
    "    for i, imgs in enumerate(collection):\n",
    "        try:\n",
    "            s = np.random.choice(range(num_per_collection[i] - (gap + 1) * seq_size), avg_num_per_class, replace=False)\n",
    "            start_ind.append(s)\n",
    "        except: # if not enough in this class\n",
    "            print(\"err\")\n",
    "            start_ind.append(np.array([]))\n",
    "    selected_classes = [i for i in range(n_collection) if start_ind[i].shape[0]>0]\n",
    "    train_ind = [[expand_start_to_seq(s) for s in ind] for ind in start_ind] # train indexes for each class\n",
    "    gd_ind = [[(x + (gap+1)//2) for x in ind_by_class] for ind_by_class in train_ind]\n",
    "    train_input = np.concatenate([np.stack([collection[i][j] for j in train_ind[i]]) for i in selected_classes], axis = 0)\n",
    "    train_gd =  np.concatenate([np.stack([collection[i][j] for j in gd_ind[i]]) for i in selected_classes], axis = 0)\n",
    "    \n",
    "    train_input, train_gd = train_input[:batch_size], train_gd[:batch_size]\n",
    "    return train_input, train_gd\n",
    "\n",
    "\n",
    "def sample_train(batch_size = 8, gap = 1, seq_size = 3): return sample(train_collection, batch_size, gap = gap, seq_size = seq_size)\n",
    "\n",
    "def sample_test(batch_size = 8, gap = 1, seq_size = 3):  return sample(test_collection, batch_size, gap, seq_size = seq_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of Image Piece Value: [-1.0, 1.0]\n",
      "seq_input shape:            (4, 6, 32, 32)\n",
      "seq_gd    shape:            (4, 6, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABD4AAACNCAYAAABfRrodAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+AAAAACq9f/RR9izX64WBAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f74783dcbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABD4AAACNCAYAAABfRrodAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f73db3df9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_sample_train(batch_size, gap, seq_size):\n",
    "    seq_input, seq_gd = sample_train(batch_size=batch_size, gap=gap, seq_size= seq_size)\n",
    "    print(\"Range of Image Piece Value: [{}, {}]\".format(np.min(seq_input), np.max(seq_input)))\n",
    "    print(\"seq_input shape:            {}\".format(seq_input.shape))\n",
    "    print(\"seq_gd    shape:            {}\".format(seq_gd.shape))\n",
    "    selected = np.random.choice(range(batch_size))\n",
    "    size = (20, 2)\n",
    "    plot_images_ndarray(seq_input[selected], title=\"Input Sequence\", size = size)\n",
    "    plot_images_ndarray(seq_gd[selected], title=\"Next Frames\", size = size)\n",
    "    \n",
    "show_sample_train(batch_size = 4, gap = 7, seq_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of Image Piece Value: [-1.0, 1.0]\n",
      "seq_input shape:            (2, 5, 32, 32)\n",
      "seq_gd    shape:            (2, 5, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABB0AAACNCAYAAAD7LALOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f73db196a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABB0AAACNCAYAAAD7LALOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBzCV7GKgseh/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f73dafbe6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_sample_test(batch_size, gap, seq_size):\n",
    "    seq_input, seq_gd = sample_test(batch_size=batch_size, gap=gap, seq_size= seq_size)\n",
    "    print(\"Range of Image Piece Value: [{}, {}]\".format(np.min(seq_input), np.max(seq_input)))\n",
    "    print(\"seq_input shape:            {}\".format(seq_input.shape))\n",
    "    print(\"seq_gd    shape:            {}\".format(seq_gd.shape))\n",
    "    selected = np.random.choice(range(batch_size))\n",
    "    size = (20, 2)\n",
    "    plot_images_ndarray(seq_input[selected], title=\"Input Sequence\", size = size)\n",
    "    plot_images_ndarray(seq_gd[selected], title=\"Next Frames\", size = size)\n",
    "    \n",
    "show_sample_test(batch_size = 2, gap = 5, seq_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_size         = 17\n",
    "feature_size     = 1024*8    # size of feature vector for LSTM\n",
    "lstm_state_size  = feature_size   # size of hidden state: [lstm_state_size, lstm_state_size]\n",
    "\n",
    "num_iteration    = 4000\n",
    "gap              = 1\n",
    "batch_size       = 8\n",
    "learning_rate    = 1.6e-4\n",
    "beta             = 0.9\n",
    "\n",
    "assert feature_size%64 == 0, \"feature_size must be divisable by 64!\"\n",
    "feature_channels = int(feature_size/8/8)\n",
    "\n",
    "model_save_path = \"trained_model/LSTM_box_64x64/1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_img(img, is_training=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        batch size of img\n",
    "    Output:\n",
    "        batch size of feature [batch_size, feature_size]\n",
    "    \"\"\"\n",
    "    x = img\n",
    "    x = tf.reshape(img, [-1, 64, 64, 1])\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters = 32, kernel_size=4, strides=2, padding='same', activation=tf.nn.relu)\n",
    "\n",
    "    x = tf.layers.conv2d(x, filters = 64, kernel_size=3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "\n",
    "    x = tf.layers.conv2d(x, filters = 64, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters = feature_channels, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "    x = tf.contrib.layers.flatten(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_seq(img_seq, seq_size = seq_size):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        img_seq: sequence of images      Tensor         [batch_size, seq_size, 32, 32]\n",
    "    Output:\n",
    "        encoded feature of the sequence  List of Tensor [batch_size, feature_size] of length seq_size\n",
    "    \"\"\"\n",
    "    img_seq = tf.transpose(img_seq, perm=[1, 0, 2, 3]) # [seq_size, batch_size, 32, 32, 1]\n",
    "    \n",
    "    return [encode_img(img_seq[i]) for i in range(seq_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(feature, is_training=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        batch size of feature [batch_size, 8, 8, feature_channel]\n",
    "    Output:\n",
    "        batch size of img [batch_size, 32, 32, 1]\n",
    "    \"\"\"\n",
    "    x = tf.reshape(feature, [-1, 16, 16, feature_channels])\n",
    "    x = tf.layers.conv2d_transpose(x, filters=16, kernel_size=2,  strides=1, activation=tf.nn.tanh, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=64, kernel_size=4, strides=2, activation=tf.nn.relu, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=32, kernel_size=3,  strides=2, activation=tf.nn.tanh, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=16, kernel_size=2,  strides=1, activation=tf.nn.tanh, padding='same')\n",
    "    img = tf.layers.conv2d_transpose(x, filters=1, kernel_size=2,  strides=1, activation=tf.nn.tanh, padding='same')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(gd_imgs, output_imgs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        gd_imgs, output_imgs: [batch_size, seq_size, 8, 8, 1]\n",
    "    Output:\n",
    "        scaler loss\n",
    "    \"\"\"\n",
    "    gd_imgs, output_imgs = tf.contrib.layers.flatten(gd_imgs), tf.contrib.layers.flatten(output_imgs)\n",
    "    return tf.norm(gd_imgs - output_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_loss(loss, seq_size = seq_size, batch_size = batch_size):\n",
    "    return loss/seq_size/batch_size/2*255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_solver(learning_rate=1e-3, beta1=0.5):\n",
    "    return tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batch_seq        = tf.placeholder(tf.float32, [None, seq_size, 64, 64], name = \"batch_seq\")\n",
    "batch_next       = tf.placeholder(tf.float32, [None, seq_size, 64, 64], name = \"batch_next\")\n",
    "is_training      = tf.placeholder(tf.bool, (), name = \"is_training\")\n",
    "\n",
    "feature_seq      = encode_seq(batch_seq)\n",
    "\n",
    "lstm_cell1       = rnn.BasicLSTMCell(lstm_state_size)\n",
    "# lstm_cell2       = rnn.BasicLSTMCell(lstm_state_size)\n",
    "# Cell             = rnn.MultiRNNCell([lstm_cell1, lstm_cell2])\n",
    "Cell = lstm_cell1\n",
    "output_feature, states = rnn.static_rnn(Cell, feature_seq, dtype=tf.float32)\n",
    "\n",
    "output_imgs = tf.stack([decode(f) for f in output_feature], axis = 1)  # [seq_size, batch_size, 32, 32, 1]\n",
    "\n",
    "loss = get_loss(batch_next, output_imgs)\n",
    "\n",
    "\n",
    "solver = get_solver(learning_rate, beta)\n",
    "\n",
    "train_step = solver.minimize(loss)\n",
    "\n",
    "# add to saver\n",
    "tf.add_to_collection('output_batch_img', output_imgs)\n",
    "tf.add_to_collection('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, train_step, loss, batch_size, num_iteration, \\\n",
    "          plot_every = 400, show_loss_every=400, num_plot = 6,  save_every = 1000):\n",
    "    losses = []\n",
    "    saver = tf.train.Saver()\n",
    "    for i in range(1, num_iteration+1):\n",
    "        # get a sample\n",
    "        # gap = np.random.choice([1,3,5,7,9])\n",
    "        seq_input, seq_gd = sample_train(batch_size, gap, seq_size = seq_size)\n",
    "        dic = {batch_seq: seq_input, batch_next: seq_gd, is_training: True}\n",
    "        \n",
    "        sess.run([train_step], dic)\n",
    "        curr_loss = sess.run(loss, dic)\n",
    "        curr_loss = scale_loss(curr_loss)# tweek loss to match report loss\n",
    "        \n",
    "        losses.append(curr_loss)\n",
    "    \n",
    "        if i%show_loss_every ==0:\n",
    "            print(\"Iteration {}:  loss = {} | Gap = {}\".format(i, curr_loss, gap))\n",
    "            \n",
    "        if i%plot_every == 0:\n",
    "            seq_input, seq_gd = sample_train(1, gap, seq_size = seq_size)\n",
    "            seq_generated = sess.run(output_imgs, feed_dict=\\\n",
    "                                      {batch_seq: seq_input, batch_next: seq_gd, is_training: False})\n",
    "            seq_generated = seq_generated[0]\n",
    "            plot_batch_images(seq_generated[:num_plot], (16, 2) , \"Iteration: {} | gap = {}\".format(i + plot_every, gap))\n",
    "        if i%save_every == 0:\n",
    "            saver.save(sess, model_save_path, global_step = i)   \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess = get_session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "losses = train(sess, train_step, loss, batch_size, num_iteration, \\\n",
    "               plot_every = 40, show_loss_every = 40, num_plot=7,  save_every = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figsize = (20, 8)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Generator Losses\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.plot(losses[-100:])\n",
    "plt.title(\"Generator Losses - Last 1000\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_loss(name, num_run = 100, gap = 3, batch_size = batch_size, seq_size = 3):\n",
    "    losses = []\n",
    "    for _ in range(num_run):\n",
    "        if name == \"train\": seq_input, seq_gd = sample_train(1, gap, seq_size = seq_size)\n",
    "        else:               seq_input, seq_gd = sample_test(1, gap, seq_size = seq_size)\n",
    "        dic = {batch_seq: seq_input, batch_next: seq_gd, is_training: True}\n",
    "        curr_loss = sess.run(loss, dic)\n",
    "        losses.append(curr_loss)\n",
    "    return scale_loss(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_generattion(seq_input):\n",
    "    feed_dict={batch_seq: seq_input, is_training: False}\n",
    "    gen_batch = sess.run(output_imgs, feed_dict)\n",
    "    return gen_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_generations(name, seq_size = 6, gap = 3):\n",
    "    if name == \"train\":  seq_input, seq_gd = sample_train(1, gap, seq_size = seq_size)\n",
    "    else:                seq_input, seq_gd = sample_test(1, gap, seq_size = seq_size)\n",
    "    \n",
    "    seq_generated = get_generattion(seq_input)\n",
    "    seq_generated, seq_input, seq_gd = seq_generated[0], seq_input[0], seq_gd[0]\n",
    "    size = (20, 2)\n",
    "    plot_images_ndarray(seq_input, title=\"Input Sequence\", size = size)\n",
    "    plot_images_ndarray(seq_gd, title=\"Next Frames - Ground Truth\", size = size)\n",
    "    plot_images_ndarray(seq_generated, title=\"Next Frames - Generated\", size = size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Evaluate on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_train(seq_size = 3, gap = 3):\n",
    "    show_generations(\"train\", seq_size, gap)\n",
    "    loss = report_loss(\"train\", 100, gap, batch_size, seq_size = seq_size)\n",
    "    print(\"Training Loss = {}\".format(loss))\n",
    "    return loss\n",
    "\n",
    "train_loss = eval_train(seq_size = seq_size, gap = gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_test(seq_size = 3, gap = 3):\n",
    "    show_generations(\"test\", seq_size, gap)\n",
    "    loss = report_loss(\"test\", 100, gap, batch_size, seq_size = seq_size)\n",
    "    print(\"Test Loss = {}\".format(loss))\n",
    "    return loss\n",
    "\n",
    "test_loss = eval_test(seq_size = seq_size, gap = gap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
