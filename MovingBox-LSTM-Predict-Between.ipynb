{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovingBox Intermediate Frame Prediction by LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from util import *\n",
    "from util.parser import *\n",
    "from util.img_kit import *\n",
    "from util.notebook_display import *\n",
    "from util.numeric_ops import *\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "from util.tf_ops import *\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "from os import walk\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0) # set default size of plots\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images:': 'triangle', 'dim': (50, 32, 32)}\n",
      "{'images:': 'rectangle', 'dim': (56, 32, 32)}\n",
      "{'images:': 'trangle-vertical', 'dim': (56, 32, 32)}\n",
      "{'images:': 'circle-diagnal', 'dim': (56, 32, 32)}\n",
      "{'images:': 'bigsquare-vertical-4', 'dim': (56, 32, 32)}\n",
      "{'images:': 'diamond', 'dim': (56, 32, 32)}\n",
      "{'images:': 'big-diamond-vertical', 'dim': (56, 32, 32)}\n",
      "{'images:': 'diamond-vertical3', 'dim': (56, 32, 32)}\n",
      "{'images:': 'square-diagnal-2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'square-vertical-4', 'dim': (56, 32, 32)}\n",
      "{'images:': 'big-diamond-diagnal1', 'dim': (56, 32, 32)}\n",
      "{'images:': 'big-diamond-vertical2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'rectangle', 'dim': (56, 32, 32)}\n",
      "{'images:': 'big-diamond-vertical3', 'dim': (56, 32, 32)}\n",
      "{'images:': 'bigSquare-vertical', 'dim': (56, 32, 32)}\n",
      "{'images:': 'bigSquare-diagnal-2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'square-vertical-2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'square-vertical-5', 'dim': (56, 32, 32)}\n",
      "{'images:': 'trangle-vertical-3', 'dim': (56, 32, 32)}\n",
      "{'images:': 'trangle-horizontal', 'dim': (56, 32, 32)}\n",
      "{'images:': 'square-vertical-3', 'dim': (56, 32, 32)}\n",
      "{'images:': 'diamond-diagnal1', 'dim': (56, 32, 32)}\n",
      "{'images:': 'bigSquare-vertical-3', 'dim': (56, 32, 32)}\n",
      "{'images:': 'big-diamond-diagnal2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'bigSquare-diagnal', 'dim': (56, 32, 32)}\n",
      "{'images:': 'trangle-vertical-2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'rectangle', 'dim': (56, 32, 32)}\n",
      "{'images:': 'bigsquare-vertical-5', 'dim': (56, 32, 32)}\n",
      "{'images:': 'diamond-vertical1', 'dim': (56, 32, 32)}\n",
      "{'images:': 'diamond-vertical2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'circle', 'dim': (56, 32, 32)}\n",
      "{'images:': 'rectangle', 'dim': (56, 32, 32)}\n",
      "{'images:': 'rectangle', 'dim': (56, 32, 32)}\n",
      "{'images:': 'rectangle', 'dim': (56, 32, 32)}\n",
      "{'images:': 'circle-diagnal-2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'square-vertical', 'dim': (56, 32, 32)}\n",
      "{'images:': 'square-diagnal', 'dim': (56, 32, 32)}\n",
      "{'images:': 'diamond-diagnal2', 'dim': (56, 32, 32)}\n",
      "\n",
      "After Augmentation: img_collections has 76 collections, 4244 images in total\n"
     ]
    }
   ],
   "source": [
    "train_collection =  get_collection(\"data/moving-box/train\")\n",
    "train_collection = augment_reverse_color(train_collection)\n",
    "train_collection = center_collections(train_collection)\n",
    "# total number of images\n",
    "total_train = sum([x.shape[0] for x in train_collection])\n",
    "print(\"\\nAfter Augmentation: img_collections has {} collections, {} images in total\".format(len(train_collection), total_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images:': 'bigSquare-vertical-2', 'dim': (56, 32, 32)}\n",
      "{'images:': 'big-diamond', 'dim': (56, 32, 32)}\n",
      "{'images:': 'rectangle', 'dim': (56, 32, 32)}\n",
      "\n",
      "After Augmentation: Test set has 3 collections, 168 images in total\n"
     ]
    }
   ],
   "source": [
    "test_collection = get_collection(\"data/moving-box/test\")\n",
    "# test_collection = augment_reverse_color(test_collection)\n",
    "test_collection = center_collections(test_collection)\n",
    "# total number of images\n",
    "total_test = sum([x.shape[0] for x in test_collection])\n",
    "print(\"\\nAfter Augmentation: Test set has {} collections, {} images in total\".format(len(test_collection), total_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(collection, batch_size = 8, gap = 1, seq_size = 3):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        collection: [img_data] - list of ndarray\n",
    "    Output:\n",
    "        (train_input, train_gd)\n",
    "        \n",
    "        train_input: [batch size, seq_size, 32, 32]\n",
    "        train_gd:    [batch size, seq_size, 32, 32]\n",
    "    \"\"\"\n",
    "    assert gap%2==1, \"Gap must be odd !\" \n",
    "    \n",
    "    def expand_start_to_seq(start_ind):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            start_ind: a number indicating index of start frame\n",
    "        Output:\n",
    "            np array of [start_ind, start_ind + gap +1, start_ind + 2*(gap+1) ...]\n",
    "        \"\"\"\n",
    "        return np.array([start_ind + i * (gap + 1) for i in range(seq_size)])\n",
    "    \n",
    "    \n",
    "    np.random.shuffle(collection)\n",
    "    # get average number of training for each class\n",
    "    n_collection = len(collection)\n",
    "    num_per_collection = [x.shape[0] for x in collection]\n",
    "    avg_num_per_class = int(np.ceil(batch_size/n_collection))\n",
    "    # start-index for each class\n",
    "    start_ind = []\n",
    "    for i, imgs in enumerate(collection):\n",
    "        try:\n",
    "            s = np.random.choice(range(num_per_collection[i] - (gap + 1) * seq_size), avg_num_per_class, replace=False)\n",
    "            start_ind.append(s)\n",
    "        except: # if not enough in this class\n",
    "            print(\"err\")\n",
    "            start_ind.append(np.array([]))\n",
    "    selected_classes = [i for i in range(n_collection) if start_ind[i].shape[0]>0]\n",
    "    train_ind = [[expand_start_to_seq(s) for s in ind] for ind in start_ind] # train indexes for each class\n",
    "    gd_ind = [[(x + (gap+1)//2) for x in ind_by_class] for ind_by_class in train_ind]\n",
    "    train_input = np.concatenate([np.stack([collection[i][j] for j in train_ind[i]]) for i in selected_classes], axis = 0)\n",
    "    train_gd =  np.concatenate([np.stack([collection[i][j] for j in gd_ind[i]]) for i in selected_classes], axis = 0)\n",
    "    \n",
    "    train_input, train_gd = train_input[:batch_size], train_gd[:batch_size]\n",
    "    return train_input, train_gd\n",
    "\n",
    "\n",
    "def sample_train(batch_size = 8, gap = 1, seq_size = 3): return sample(train_collection, batch_size, gap = gap, seq_size = seq_size)\n",
    "\n",
    "def sample_test(batch_size = 8, gap = 1, seq_size = 3):  return sample(test_collection, batch_size, gap, seq_size = seq_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of Image Piece Value: [-1.0, 1.0]\n",
      "seq_input shape:            (4, 6, 32, 32)\n",
      "seq_gd    shape:            (4, 6, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABD4AAACNCAYAAABfRrodAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1591025ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABD4AAACNCAYAAABfRrodAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/R8SWsi5PEFt2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f158ef1e2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_sample_train(batch_size, gap, seq_size):\n",
    "    seq_input, seq_gd = sample_train(batch_size=batch_size, gap=gap, seq_size= seq_size)\n",
    "    print(\"Range of Image Piece Value: [{}, {}]\".format(np.min(seq_input), np.max(seq_input)))\n",
    "    print(\"seq_input shape:            {}\".format(seq_input.shape))\n",
    "    print(\"seq_gd    shape:            {}\".format(seq_gd.shape))\n",
    "    selected = np.random.choice(range(batch_size))\n",
    "    size = (20, 2)\n",
    "    plot_images_ndarray(seq_input[selected], title=\"Input Sequence\", size = size)\n",
    "    plot_images_ndarray(seq_gd[selected], title=\"Next Frames\", size = size)\n",
    "    \n",
    "show_sample_train(batch_size = 4, gap = 7, seq_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of Image Piece Value: [-1.0, 1.0]\n",
      "seq_input shape:            (2, 5, 32, 32)\n",
      "seq_gd    shape:            (2, 5, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABB0AAACNCAYAAAD7LALOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f158efde630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABB0AAACNCAYAAAD7LALOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAKALoQMAAADQxf8BVIDBII0v5bwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f158ef5efd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_sample_test(batch_size, gap, seq_size):\n",
    "    seq_input, seq_gd = sample_test(batch_size=batch_size, gap=gap, seq_size= seq_size)\n",
    "    print(\"Range of Image Piece Value: [{}, {}]\".format(np.min(seq_input), np.max(seq_input)))\n",
    "    print(\"seq_input shape:            {}\".format(seq_input.shape))\n",
    "    print(\"seq_gd    shape:            {}\".format(seq_gd.shape))\n",
    "    selected = np.random.choice(range(batch_size))\n",
    "    size = (20, 2)\n",
    "    plot_images_ndarray(seq_input[selected], title=\"Input Sequence\", size = size)\n",
    "    plot_images_ndarray(seq_gd[selected], title=\"Next Frames\", size = size)\n",
    "    \n",
    "show_sample_test(batch_size = 2, gap = 5, seq_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_size         = 9\n",
    "feature_size     = 1024*6    # size of feature vector for LSTM\n",
    "lstm_state_size  = feature_size   # size of hidden state: [lstm_state_size, lstm_state_size]\n",
    "\n",
    "num_iteration    = 1000\n",
    "gap              = 1\n",
    "batch_size       = 32\n",
    "learning_rate    = 1.6e-4\n",
    "beta             = 0.9\n",
    "\n",
    "assert feature_size%64 == 0, \"feature_size must be divisable by 64!\"\n",
    "feature_channels = int(feature_size/8/8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_img(img, is_training=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        batch size of img\n",
    "    Output:\n",
    "        batch size of feature [batch_size, feature_size]\n",
    "    \"\"\"\n",
    "    x = img\n",
    "    x = tf.reshape(img, [-1, 32, 32, 1])\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters = 64, kernel_size=4, strides=2, padding='same', activation=tf.nn.relu)\n",
    "\n",
    "    x = tf.layers.conv2d(x, filters = 64, kernel_size=3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "\n",
    "    x = tf.layers.conv2d(x, filters = 64, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters = feature_channels, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "    x = tf.contrib.layers.flatten(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_seq(img_seq, seq_size = seq_size):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        img_seq: sequence of images      Tensor         [batch_size, seq_size, 32, 32]\n",
    "    Output:\n",
    "        encoded feature of the sequence  List of Tensor [batch_size, feature_size] of length seq_size\n",
    "    \"\"\"\n",
    "    img_seq = tf.transpose(img_seq, perm=[1, 0, 2, 3]) # [seq_size, batch_size, 32, 32, 1]\n",
    "    \n",
    "    return [encode_img(img_seq[i]) for i in range(seq_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(feature, is_training=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        batch size of feature [batch_size, 8, 8, feature_channel]\n",
    "    Output:\n",
    "        batch size of img [batch_size, 32, 32, 1]\n",
    "    \"\"\"\n",
    "    x = tf.reshape(feature, [-1, 8, 8, feature_channels])\n",
    "    x = tf.layers.conv2d_transpose(x, filters=16, kernel_size=2,  strides=1, activation=tf.nn.tanh, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=64, kernel_size=4, strides=2, activation=tf.nn.relu, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=32, kernel_size=3,  strides=2, activation=tf.nn.tanh, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=16, kernel_size=2,  strides=1, activation=tf.nn.tanh, padding='same')\n",
    "    img = tf.layers.conv2d_transpose(x, filters=1, kernel_size=2,  strides=1, activation=tf.nn.tanh, padding='same')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(gd_imgs, output_imgs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        gd_imgs, output_imgs: [batch_size, seq_size, 8, 8, 1]\n",
    "    Output:\n",
    "        scaler loss\n",
    "    \"\"\"\n",
    "    gd_imgs, output_imgs = tf.contrib.layers.flatten(gd_imgs), tf.contrib.layers.flatten(output_imgs)\n",
    "    return tf.norm(gd_imgs - output_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_loss(loss, seq_size = seq_size, batch_size = batch_size):\n",
    "    return loss/seq_size/batch_size/2*255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_solver(learning_rate=1e-3, beta1=0.5):\n",
    "    return tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batch_seq        = tf.placeholder(tf.float32, [None, seq_size, 32, 32])\n",
    "batch_next       = tf.placeholder(tf.float32, [None, seq_size, 32, 32])\n",
    "is_training      = tf.placeholder(tf.bool, ())\n",
    "\n",
    "feature_seq      = encode_seq(batch_seq)\n",
    "\n",
    "lstm_cell1       = rnn.BasicLSTMCell(lstm_state_size)\n",
    "# lstm_cell2       = rnn.BasicLSTMCell(lstm_state_size)\n",
    "# Cell             = rnn.MultiRNNCell([lstm_cell1, lstm_cell2])\n",
    "Cell = lstm_cell1\n",
    "output_feature, states = rnn.static_rnn(Cell, feature_seq, dtype=tf.float32)\n",
    "\n",
    "output_imgs = tf.stack([decode(f) for f in output_feature], axis = 1)  # [seq_size, batch_size, 32, 32, 1]\n",
    "\n",
    "loss = get_loss(batch_next, output_imgs)\n",
    "\n",
    "\n",
    "solver = get_solver(learning_rate, beta)\n",
    "\n",
    "train_step = solver.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, train_step, loss, batch_size, num_iteration, plot_every = 400, show_loss_every=400, num_plot = 6):\n",
    "    losses = []\n",
    "    for i in range(num_iteration):\n",
    "        # get a sample\n",
    "        # gap = np.random.choice([1,3,5,7,9])\n",
    "        seq_input, seq_gd = sample_train(batch_size, gap, seq_size = seq_size)\n",
    "        dic = {batch_seq: seq_input, batch_next: seq_gd, is_training: True}\n",
    "        \n",
    "        sess.run([train_step], dic)\n",
    "        curr_loss = sess.run(loss, dic)\n",
    "        curr_loss = scale_loss(curr_loss)# tweek loss to match report loss\n",
    "        \n",
    "        losses.append(curr_loss)\n",
    "    \n",
    "        if i%show_loss_every ==0:\n",
    "            print(\"Iteration {}:  loss = {} | Gap = {}\".format(i, curr_loss, gap))\n",
    "            \n",
    "        if i%plot_every == 0:\n",
    "            seq_input, seq_gd = sample_train(1, gap, seq_size = seq_size)\n",
    "            seq_generated = sess.run(output_imgs, feed_dict=\\\n",
    "                                      {batch_seq: seq_input, batch_next: seq_gd, is_training: False})\n",
    "            seq_generated = seq_generated[0]\n",
    "            plot_batch_images(seq_generated[:num_plot], (16, 2) , \"Iteration: {} | gap = {}\".format(i + plot_every, gap))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess = get_session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "losses = train(sess, train_step, loss, batch_size, num_iteration, plot_every = 40, show_loss_every = 40, num_plot=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figsize = (20, 8)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Generator Losses\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.plot(losses[-100:])\n",
    "plt.title(\"Generator Losses - Last 1000\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_loss(name, num_run = 100, gap = 3, batch_size = batch_size, seq_size = 3):\n",
    "    losses = []\n",
    "    for _ in range(num_run):\n",
    "        if name == \"train\": seq_input, seq_gd = sample_train(1, gap, seq_size = seq_size)\n",
    "        else:               seq_input, seq_gd = sample_test(1, gap, seq_size = seq_size)\n",
    "        dic = {batch_seq: seq_input, batch_next: seq_gd, is_training: True}\n",
    "        curr_loss = sess.run(loss, dic)\n",
    "        losses.append(curr_loss)\n",
    "    return scale_loss(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_generattion(seq_input):\n",
    "    feed_dict={batch_seq: seq_input, is_training: False}\n",
    "    gen_batch = sess.run(output_imgs, feed_dict)\n",
    "    return gen_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_generations(name, seq_size = 6, gap = 3):\n",
    "    if name == \"train\":  seq_input, seq_gd = sample_train(1, gap, seq_size = seq_size)\n",
    "    else:                seq_input, seq_gd = sample_test(1, gap, seq_size = seq_size)\n",
    "    \n",
    "    seq_generated = get_generattion(seq_input)\n",
    "    seq_generated, seq_input, seq_gd = seq_generated[0], seq_input[0], seq_gd[0]\n",
    "    size = (20, 2)\n",
    "    plot_images_ndarray(seq_input, title=\"Input Sequence\", size = size)\n",
    "    plot_images_ndarray(seq_gd, title=\"Next Frames - Ground Truth\", size = size)\n",
    "    plot_images_ndarray(seq_generated, title=\"Next Frames - Generated\", size = size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Evaluate on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_train(seq_size = 3, gap = 3):\n",
    "    show_generations(\"train\", seq_size, gap)\n",
    "    loss = report_loss(\"train\", 100, gap, batch_size, seq_size = seq_size)\n",
    "    print(\"Training Loss = {}\".format(loss))\n",
    "    return loss\n",
    "\n",
    "train_loss = eval_train(seq_size = seq_size, gap = gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_test(seq_size = 3, gap = 3):\n",
    "    show_generations(\"test\", seq_size, gap)\n",
    "    loss = report_loss(\"test\", 100, gap, batch_size, seq_size = seq_size)\n",
    "    print(\"Test Loss = {}\".format(loss))\n",
    "    return loss\n",
    "\n",
    "test_loss = eval_test(seq_size = seq_size, gap = gap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
