{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovingBox Intermediate Frame Prediction by LSTM | Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from util import *\n",
    "from util.parser import *\n",
    "from util.img_kit import *\n",
    "from util.notebook_display import *\n",
    "from util.numeric_ops import *\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "from util.tf_ops import *\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "from os import walk\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from time import time\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0) # set default size of plots\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images:': 'square-square-5', 'dim': (41, 32, 32)}\n",
      "{'images:': 'big-circle-circle-4', 'dim': (49, 32, 32)}\n",
      "{'images:': 'square-square-1', 'dim': (41, 32, 32)}\n",
      "{'images:': 'circle-spiral-1', 'dim': (97, 32, 32)}\n",
      "{'images:': 'bigsquare-vertical-4', 'dim': (56, 32, 32)}\n",
      "{'images:': 'circle-spiral-2', 'dim': (97, 32, 32)}\n",
      "{'images:': 'square-square-3', 'dim': (41, 32, 32)}\n",
      "{'images:': 'square-vertical-5', 'dim': (56, 32, 32)}\n",
      "{'images:': 'big-circle-circle-1', 'dim': (49, 32, 32)}\n",
      "{'images:': 'big-circle-circle-2', 'dim': (49, 32, 32)}\n",
      "{'images:': 'circle-spiral-4', 'dim': (97, 32, 32)}\n",
      "{'images:': 'big-circle-circle-3', 'dim': (49, 32, 32)}\n",
      "{'images:': 'circle-spiral-3', 'dim': (97, 32, 32)}\n",
      "{'images:': 'circle-spiral-5', 'dim': (97, 32, 32)}\n",
      "{'images:': 'square-square-2', 'dim': (41, 32, 32)}\n",
      "{'images:': 'square-vertical', 'dim': (56, 32, 32)}\n",
      "\n",
      "After Augmentation: img_collections has 64 collections, 4052 images in total\n"
     ]
    }
   ],
   "source": [
    "train_collection =  get_collection(\"../data/moving-box/multi-path/train\")\n",
    "train_collection = augment_reverse_color(train_collection)\n",
    "train_collection = augment_reverse_sequence(train_collection)\n",
    "train_collection = center_collections(train_collection)\n",
    "\n",
    "# total number of images\n",
    "total_train = sum([x.shape[0] for x in train_collection])\n",
    "print(\"\\nAfter Augmentation: img_collections has {} collections, {} images in total\".format(len(train_collection), total_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images:': 'big-circle-circle-5', 'dim': (49, 32, 32)}\n",
      "{'images:': 'square-vertical-4', 'dim': (56, 32, 32)}\n",
      "{'images:': 'square-square-4', 'dim': (41, 32, 32)}\n",
      "{'images:': 'circle-spiral-6', 'dim': (97, 32, 32)}\n",
      "\n",
      "After Augmentation: Test set has 8 collections, 486 images in total\n"
     ]
    }
   ],
   "source": [
    "test_collection = get_collection(\"../data/moving-box/multi-path/test\")\n",
    "test_collection = augment_reverse_color(test_collection)\n",
    "test_collection = center_collections(test_collection)\n",
    "# total number of images\n",
    "total_test = sum([x.shape[0] for x in test_collection])\n",
    "print(\"\\nAfter Augmentation: Test set has {} collections, {} images in total\".format(len(test_collection), total_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(collection, batch_size = 8, gap = 1, seq_size = 3):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        collection: [img_data] - list of ndarray\n",
    "    Output:\n",
    "        (train_input, train_gd)\n",
    "        \n",
    "        train_input: [batch size, seq_size, 32, 32]\n",
    "        train_gd:    [batch size, seq_size, 32, 32]\n",
    "    \"\"\"\n",
    "    assert gap%2==1, \"Gap must be odd !\" \n",
    "    \n",
    "    def expand_start_to_seq(start_ind):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            start_ind: a number indicating index of start frame\n",
    "        Output:\n",
    "            np array of [start_ind, start_ind + gap +1, start_ind + 2*(gap+1) ...]\n",
    "        \"\"\"\n",
    "        return np.array([start_ind + i * (gap + 1) for i in range(seq_size)])\n",
    "    \n",
    "    \n",
    "    np.random.shuffle(collection)\n",
    "    # get average number of training for each class\n",
    "    n_collection = len(collection)\n",
    "    num_per_collection = [x.shape[0] for x in collection]\n",
    "    avg_num_per_class = int(np.ceil(batch_size/n_collection))\n",
    "    # start-index for each class\n",
    "    start_ind = []\n",
    "    for i, imgs in enumerate(collection):\n",
    "        try:\n",
    "            s = np.random.choice(range(num_per_collection[i] - (gap + 1) * seq_size), avg_num_per_class, replace=False)\n",
    "            start_ind.append(s)\n",
    "        except: # if not enough in this class\n",
    "            print(\"err\")\n",
    "            start_ind.append(np.array([]))\n",
    "    selected_classes = [i for i in range(n_collection) if start_ind[i].shape[0]>0]\n",
    "    train_ind = [[expand_start_to_seq(s) for s in ind] for ind in start_ind] # train indexes for each class\n",
    "    gd_ind = [[(x + (gap+1)//2) for x in ind_by_class] for ind_by_class in train_ind]\n",
    "    train_input = np.concatenate([np.stack([collection[i][j] for j in train_ind[i]]) for i in selected_classes], axis = 0)\n",
    "    train_gd =  np.concatenate([np.stack([collection[i][j] for j in gd_ind[i]]) for i in selected_classes], axis = 0)\n",
    "    \n",
    "    train_input, train_gd = train_input[:batch_size], train_gd[:batch_size]\n",
    "    return train_input, train_gd\n",
    "\n",
    "\n",
    "def sample_train(batch_size = 8, gap = 1, seq_size = 3): return sample(train_collection, batch_size, gap = gap, seq_size = seq_size)\n",
    "\n",
    "def sample_test(batch_size = 8, gap = 1, seq_size = 3):  return sample(test_collection, batch_size, gap, seq_size = seq_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of Image Piece Value: [-1.0, 1.0]\n",
      "seq_input shape:            (4, 8, 32, 32)\n",
      "seq_gd    shape:            (4, 8, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGcAAACNCAYAAAAaRxefAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkXAGAAAAoCPhDAAAAEBH/wdT7OG6C4kNCgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2fb499fa58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGcAAACNCAYAAAAaRxefAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f304d748c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_sample_train(batch_size, gap, seq_size):\n",
    "    seq_input, seq_gd = sample_train(batch_size=batch_size, gap=gap, seq_size= seq_size)\n",
    "    print(\"Range of Image Piece Value: [{}, {}]\".format(np.min(seq_input), np.max(seq_input)))\n",
    "    print(\"seq_input shape:            {}\".format(seq_input.shape))\n",
    "    print(\"seq_gd    shape:            {}\".format(seq_gd.shape))\n",
    "    selected = np.random.choice(range(batch_size))\n",
    "    size = (20, 2)\n",
    "    plot_images_ndarray(seq_input[selected], title=\"Input Sequence\", size = size)\n",
    "    plot_images_ndarray(seq_gd[selected], title=\"Next Frames\", size = size)\n",
    "    \n",
    "show_sample_train(batch_size = 4, gap = 3, seq_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of Image Piece Value: [-1.0, 1.0]\n",
      "seq_input shape:            (2, 5, 32, 32)\n",
      "seq_gd    shape:            (2, 5, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABB0AAACNCAYAAAD7LALOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wCCH8CgYfC76QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2fb1fd3ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABB0AAACNCAYAAAD7LALOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAKoQOgAAAABVCB0AAACAKv4PuxtPYNISeWIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2fb22511d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_sample_test(batch_size, gap, seq_size):\n",
    "    seq_input, seq_gd = sample_test(batch_size=batch_size, gap=gap, seq_size= seq_size)\n",
    "    print(\"Range of Image Piece Value: [{}, {}]\".format(np.min(seq_input), np.max(seq_input)))\n",
    "    print(\"seq_input shape:            {}\".format(seq_input.shape))\n",
    "    print(\"seq_gd    shape:            {}\".format(seq_gd.shape))\n",
    "    selected = np.random.choice(range(batch_size))\n",
    "    size = (20, 2)\n",
    "    plot_images_ndarray(seq_input[selected], title=\"Input Sequence\", size = size)\n",
    "    plot_images_ndarray(seq_gd[selected], title=\"Next Frames\", size = size)\n",
    "    \n",
    "show_sample_test(batch_size = 2, gap = 5, seq_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_size         = 9\n",
    "feature_size     = 1024*6    # size of feature vector for LSTM\n",
    "lstm_state_size  = feature_size   # size of hidden state: [lstm_state_size, lstm_state_size]\n",
    "\n",
    "num_iteration    = 1000\n",
    "gap              = 1\n",
    "batch_size       = 8\n",
    "learning_rate    = 2e-3\n",
    "beta             = 0.9\n",
    "\n",
    "assert feature_size%64 == 0, \"feature_size must be divisable by 64!\"\n",
    "feature_channels = int(feature_size/8/8)\n",
    "\n",
    "model_save_path = \"../trained_model/LSTM_box_32x32/{}/\".format(time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Directory for Model to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model to be saved at ../trained_model/LSTM_box_32x32/1496678208.2885897/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(model_save_path)\n",
    "    print(\"Model to be saved at {}\".format(model_save_path))\n",
    "except:\n",
    "    assert \"Cannot create save folder!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_img(img, is_training=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        batch size of img\n",
    "    Output:\n",
    "        batch size of feature [batch_size, feature_size]\n",
    "    \"\"\"\n",
    "    x = img\n",
    "    x = tf.reshape(img, [-1, 32, 32, 1])\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters = 32, kernel_size=4, strides=2, padding='same', activation=tf.nn.relu)\n",
    "\n",
    "    x = tf.layers.conv2d(x, filters = 32, kernel_size=3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "\n",
    "    x = tf.layers.conv2d(x, filters = 64, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters = feature_channels, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "    x = tf.contrib.layers.flatten(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_seq(img_seq, seq_size = seq_size):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        img_seq: sequence of images      Tensor         [batch_size, seq_size, 32, 32]\n",
    "    Output:\n",
    "        encoded feature of the sequence  List of Tensor [batch_size, feature_size] of length seq_size\n",
    "    \"\"\"\n",
    "    img_seq = tf.transpose(img_seq, perm=[1, 0, 2, 3]) # [seq_size, batch_size, 32, 32, 1]\n",
    "    \n",
    "    return [encode_img(img_seq[i]) for i in range(seq_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(feature, is_training=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        batch size of feature [batch_size, 8, 8, feature_channel]\n",
    "    Output:\n",
    "        batch size of img [batch_size, 32, 32, 1]\n",
    "    \"\"\"\n",
    "    x = tf.reshape(feature, [-1, 8, 8, feature_channels])\n",
    "    x = tf.layers.conv2d_transpose(x, filters=16, kernel_size=2,  strides=1, activation=tf.nn.tanh, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=32, kernel_size=4, strides=2, activation=tf.nn.relu, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=32, kernel_size=3,  strides=2, activation=tf.nn.tanh, padding='same')\n",
    "    x = tf.layers.conv2d_transpose(x, filters=16, kernel_size=2,  strides=1, activation=tf.nn.tanh, padding='same')\n",
    "    img = tf.layers.conv2d_transpose(x, filters=1, kernel_size=2,  strides=1, activation=tf.nn.tanh, padding='same')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(gd_imgs, output_imgs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        gd_imgs, output_imgs: [batch_size, seq_size, 8, 8, 1]\n",
    "    Output:\n",
    "        scaler loss\n",
    "    \"\"\"\n",
    "    gd_imgs, output_imgs = tf.contrib.layers.flatten(gd_imgs), tf.contrib.layers.flatten(output_imgs)\n",
    "    return tf.norm(gd_imgs - output_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_loss(loss, seq_size = seq_size, batch_size = batch_size):\n",
    "    return loss/seq_size/batch_size/2*255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_solver(learning_rate=1e-3, beta1=0.5):\n",
    "    return tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batch_seq        = tf.placeholder(tf.float32, [None, seq_size, 32, 32], name = \"batch_seq\")\n",
    "batch_next       = tf.placeholder(tf.float32, [None, seq_size, 32, 32], name = \"batch_next\")\n",
    "is_training      = tf.placeholder(tf.bool, (), name = \"is_training\")\n",
    "\n",
    "feature_seq      = encode_seq(batch_seq)\n",
    "\n",
    "lstm_cell1       = rnn.BasicLSTMCell(lstm_state_size)\n",
    "# lstm_cell2       = rnn.BasicLSTMCell(lstm_state_size)\n",
    "# Cell             = rnn.MultiRNNCell([lstm_cell1, lstm_cell2])\n",
    "Cell = lstm_cell1\n",
    "output_feature, states = rnn.static_rnn(Cell, feature_seq, dtype=tf.float32)\n",
    "\n",
    "output_imgs = tf.stack([decode(f) for f in output_feature], axis = 1)  # [seq_size, batch_size, 32, 32, 1]\n",
    "\n",
    "loss = get_loss(batch_next, output_imgs)\n",
    "\n",
    "\n",
    "solver = get_solver(learning_rate, beta)\n",
    "\n",
    "train_step = solver.minimize(loss)\n",
    "\n",
    "# add to saver\n",
    "tf.add_to_collection('output_batch_img', output_imgs)\n",
    "tf.add_to_collection('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, train_step, loss, batch_size, num_iteration, \\\n",
    "          plot_every = 400, show_loss_every=400, num_plot = 6,  save_every = 1000):\n",
    "    losses = []\n",
    "    saver = tf.train.Saver()\n",
    "    for i in range(1, num_iteration+1):\n",
    "        # get a sample\n",
    "#         gap = np.random.choice([1,3,5,7,9])\n",
    "        seq_input, seq_gd = sample_train(batch_size, gap, seq_size = seq_size)\n",
    "        dic = {batch_seq: seq_input, batch_next: seq_gd, is_training: True}\n",
    "        \n",
    "        sess.run([train_step], dic)\n",
    "        curr_loss = sess.run(loss, dic)\n",
    "        curr_loss = scale_loss(curr_loss)# tweek loss to match report loss\n",
    "        \n",
    "        losses.append(curr_loss)\n",
    "    \n",
    "        if i%show_loss_every ==0:\n",
    "            print(\"Iteration {}:  loss = {} | Gap = {}\".format(i, curr_loss, gap))\n",
    "            \n",
    "        if i%plot_every == 0:\n",
    "            seq_input, seq_gd = sample_train(1, gap, seq_size = seq_size)\n",
    "            seq_generated = sess.run(output_imgs, feed_dict=\\\n",
    "                                      {batch_seq: seq_input, batch_next: seq_gd, is_training: False})\n",
    "            seq_generated = seq_generated[0]\n",
    "            plot_batch_images(seq_generated[:num_plot], (16, 2) , \"Iteration: {} | gap = {}\".format(i + plot_every, gap))\n",
    "        if i%save_every == 0:\n",
    "            saver.save(sess, model_save_path, global_step = i)   \n",
    "    save_learning_curve(iterations = list(range(1, num_iteration+1)), loss = losses, \\\n",
    "                            save_path = \"output/learning_curve/box_LSTM-64x64-{}\".format(time()))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[12288,24576]\n\t [[Node: gradients/rnn/basic_lstm_cell_6/basic_lstm_cell/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](rnn/basic_lstm_cell_6/basic_lstm_cell/concat, gradients/rnn/basic_lstm_cell_6/BiasAdd_grad/tuple/control_dependency)]]\n\nCaused by op 'gradients/rnn/basic_lstm_cell_6/basic_lstm_cell/MatMul_grad/MatMul_1', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 405, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-309d61c8838e>\", line 22, in <module>\n    train_step = solver.minimize(loss)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 279, in minimize\n    grad_loss=grad_loss)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 345, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 482, in gradients\n    in_grads = grad_fn(op, *out_grads)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py\", line 768, in _MatMulGrad\n    op.inputs[0], grad, transpose_a=True))\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1765, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1454, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'rnn/basic_lstm_cell_6/basic_lstm_cell/MatMul', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 17 identical lines from previous traceback]\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-309d61c8838e>\", line 13, in <module>\n    output_feature, states = rnn.static_rnn(Cell, feature_seq, dtype=tf.float32)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\", line 197, in static_rnn\n    (output, state) = call_cell()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\", line 184, in <lambda>\n    call_cell = lambda: cell(input_, state)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 179, in __call__\n    concat = _linear([inputs, h], 4 * self._num_units, True, scope=scope)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 751, in _linear\n    res = math_ops.matmul(array_ops.concat(args, 1), weights)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1765, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[12288,24576]\n\t [[Node: gradients/rnn/basic_lstm_cell_6/basic_lstm_cell/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](rnn/basic_lstm_cell_6/basic_lstm_cell/concat, gradients/rnn/basic_lstm_cell_6/BiasAdd_grad/tuple/control_dependency)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[12288,24576]\n\t [[Node: gradients/rnn/basic_lstm_cell_6/basic_lstm_cell/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](rnn/basic_lstm_cell_6/basic_lstm_cell/concat, gradients/rnn/basic_lstm_cell_6/BiasAdd_grad/tuple/control_dependency)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3b4db140b41f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m,\u001b[0m                \u001b[0mplot_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_loss_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_plot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0msave_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-1c9afd4a5406>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(sess, train_step, loss, batch_size, num_iteration, plot_every, show_loss_every, num_plot, save_every)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mdic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mbatch_seq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mseq_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_next\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mseq_gd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mcurr_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mcurr_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscale_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# tweek loss to match report loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1035\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[12288,24576]\n\t [[Node: gradients/rnn/basic_lstm_cell_6/basic_lstm_cell/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](rnn/basic_lstm_cell_6/basic_lstm_cell/concat, gradients/rnn/basic_lstm_cell_6/BiasAdd_grad/tuple/control_dependency)]]\n\nCaused by op 'gradients/rnn/basic_lstm_cell_6/basic_lstm_cell/MatMul_grad/MatMul_1', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 405, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-309d61c8838e>\", line 22, in <module>\n    train_step = solver.minimize(loss)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 279, in minimize\n    grad_loss=grad_loss)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 345, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 482, in gradients\n    in_grads = grad_fn(op, *out_grads)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py\", line 768, in _MatMulGrad\n    op.inputs[0], grad, transpose_a=True))\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1765, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1454, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'rnn/basic_lstm_cell_6/basic_lstm_cell/MatMul', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 17 identical lines from previous traceback]\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-309d61c8838e>\", line 13, in <module>\n    output_feature, states = rnn.static_rnn(Cell, feature_seq, dtype=tf.float32)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\", line 197, in static_rnn\n    (output, state) = call_cell()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\", line 184, in <lambda>\n    call_cell = lambda: cell(input_, state)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 179, in __call__\n    concat = _linear([inputs, h], 4 * self._num_units, True, scope=scope)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 751, in _linear\n    res = math_ops.matmul(array_ops.concat(args, 1), weights)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1765, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[12288,24576]\n\t [[Node: gradients/rnn/basic_lstm_cell_6/basic_lstm_cell/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](rnn/basic_lstm_cell_6/basic_lstm_cell/concat, gradients/rnn/basic_lstm_cell_6/BiasAdd_grad/tuple/control_dependency)]]\n"
     ]
    }
   ],
   "source": [
    "sess = get_session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "losses = train(sess, train_step, loss, batch_size, num_iteration, \\\n",
    "               plot_every = 40, show_loss_every = 40, num_plot=7,  save_every = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figsize = (20, 8)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Generator Losses\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.plot(losses[-100:])\n",
    "plt.title(\"Generator Losses - Last 1000\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_loss(name, num_run = 100, gap = 3, batch_size = batch_size, seq_size = 3):\n",
    "    losses = []\n",
    "    for _ in range(num_run):\n",
    "        if name == \"train\": seq_input, seq_gd = sample_train(1, gap, seq_size = seq_size)\n",
    "        else:               seq_input, seq_gd = sample_test(1, gap, seq_size = seq_size)\n",
    "        dic = {batch_seq: seq_input, batch_next: seq_gd, is_training: True}\n",
    "        curr_loss = sess.run(loss, dic)\n",
    "        losses.append(curr_loss)\n",
    "    return scale_loss(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_generattion(seq_input):\n",
    "    feed_dict={batch_seq: seq_input, is_training: False}\n",
    "    gen_batch = sess.run(output_imgs, feed_dict)\n",
    "    return gen_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_generations(name, seq_size = 6, gap = 3):\n",
    "    if name == \"train\":  seq_input, seq_gd = sample_train(1, gap, seq_size = seq_size)\n",
    "    else:                seq_input, seq_gd = sample_test(1, gap, seq_size = seq_size)\n",
    "    \n",
    "    seq_generated = get_generattion(seq_input)\n",
    "    seq_generated, seq_input, seq_gd = seq_generated[0], seq_input[0], seq_gd[0]\n",
    "    size = (20, 2)\n",
    "    plot_images_ndarray(seq_input, title=\"Input Sequence\", size = size)\n",
    "    plot_images_ndarray(seq_gd, title=\"Next Frames - Ground Truth\", size = size)\n",
    "    plot_images_ndarray(seq_generated, title=\"Next Frames - Generated\", size = size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Evaluate on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_train(seq_size = 3, gap = 3):\n",
    "    show_generations(\"train\", seq_size, gap)\n",
    "    loss = report_loss(\"train\", 100, gap, batch_size, seq_size = seq_size)\n",
    "    print(\"Training Loss = {}\".format(loss))\n",
    "    return loss\n",
    "\n",
    "train_loss = eval_train(seq_size = seq_size, gap = gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_test(seq_size = 3, gap = 3):\n",
    "    show_generations(\"test\", seq_size, gap)\n",
    "    loss = report_loss(\"test\", 100, gap, batch_size, seq_size = seq_size)\n",
    "    print(\"Test Loss = {}\".format(loss))\n",
    "    return loss\n",
    "\n",
    "test_loss = eval_test(seq_size = seq_size, gap = gap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
