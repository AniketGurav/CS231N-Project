{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Stream Generation \n",
    "### Backgroun + Foreground\n",
    "#### Reference: MIT Generating Video with Scene Dynamics[http://web.mit.edu/vondrick/tinyvideo/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from util import *\n",
    "from util.parser import *\n",
    "from util.img_kit import *\n",
    "from util.tf_ops import *\n",
    "from util.notebook_display import *\n",
    "from util.numeric_ops import *\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "from os import walk\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0) # set default size of plots\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': (50, 32, 32), 'images:': 'triangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'trangle-vertical'}\n",
      "{'dim': (56, 32, 32), 'images:': 'circle-diagnal'}\n",
      "{'dim': (56, 32, 32), 'images:': 'square-diagnal-2'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'square-vertical-2'}\n",
      "{'dim': (56, 32, 32), 'images:': 'trangle-vertical-3'}\n",
      "{'dim': (56, 32, 32), 'images:': 'trangle-horizontal'}\n",
      "{'dim': (56, 32, 32), 'images:': 'square-vertical-3'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'trangle-vertical-2'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (14, 32, 32), 'images:': 'moving box uniform'}\n",
      "{'dim': (56, 32, 32), 'images:': 'circle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'circle-diagnal-2'}\n",
      "{'dim': (56, 32, 32), 'images:': 'square-vertical'}\n",
      "{'dim': (56, 32, 32), 'images:': 'square-diagnal'}\n",
      "\n",
      "After Augmentation: img_collections has 21 collections, 1128 images in total\n"
     ]
    }
   ],
   "source": [
    "img_collections = get_processed_moving_box()\n",
    "# number of images for each collection\n",
    "num_per_collection = [x.shape[0] for x in img_collections]\n",
    "# number of collections\n",
    "n_collection = len(img_collections)\n",
    "# total number of images\n",
    "total_imgs = sum(num_per_collection)\n",
    "print(\"\\nAfter Augmentation: img_collections has {} collections, {} images in total\".format(n_collection, total_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ind_collection, val_ind_collection = zip(*[split_train_dev(x) for x in img_collections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(batch_size = 8, train=True, gap = 1):\n",
    "    # get average number of training for each class\n",
    "    avg_num_per_class = int(np.ceil(batch_size/n_collection))\n",
    "    # before-index for each class\n",
    "    before_ind = []\n",
    "    for i in range(n_collection):\n",
    "        data = train_ind_collection[i] if train else val_ind_collection[i]\n",
    "        try:\n",
    "            s = np.random.choice(list(filter(lambda x:x<num_per_collection[i]-gap-1, data)),avg_num_per_class, replace=False)\n",
    "            before_ind.append(s)\n",
    "        except:\n",
    "            before_ind.append(np.array([]))\n",
    "    # after-index for each class\n",
    "    after_ind = [x+gap+1 for x in before_ind]\n",
    "    # mid-index for each class\n",
    "    mid_ind = [x+(gap+1)//2 for x in before_ind]\n",
    "    \n",
    "    selected_classed = [i for i in range(n_collection) if before_ind[i].shape[0]>0]\n",
    "    before_imgs = np.concatenate([img_collections[i][before_ind[i]] for i in selected_classed], axis = 0)\n",
    "    after_imgs = np.concatenate([img_collections[i][after_ind[i]] for i in selected_classed], axis = 0)\n",
    "    mid_imgs = np.concatenate([img_collections[i][mid_ind[i]] for i in selected_classed], axis = 0)\n",
    "    \n",
    "    clipped = np.random.choice(range(before_imgs.shape[0]), batch_size, replace=False)\n",
    "    before_imgs = before_imgs[clipped]\n",
    "    mid_imgs = mid_imgs[clipped]\n",
    "    after_imgs = after_imgs[clipped]\n",
    "    return (before_imgs, after_imgs), mid_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_train(batch_size = 8, gap = 1):\n",
    "    return sample(batch_size)\n",
    "def sample_dev(batch_size = 8, gap = 1):\n",
    "    return sample(batch_size, False, gap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
