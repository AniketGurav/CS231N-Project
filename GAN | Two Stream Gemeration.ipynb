{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Stream Generation \n",
    "### Backgroun + Foreground\n",
    "#### Reference: MIT Generating Video with Scene Dynamics[http://web.mit.edu/vondrick/tinyvideo/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from util import *\n",
    "from util.parser import *\n",
    "from util.img_kit import *\n",
    "from util.tf_ops import *\n",
    "from util.notebook_display import *\n",
    "from util.numeric_ops import *\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "from os import walk\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0) # set default size of plots\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': (50, 32, 32), 'images:': 'triangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'trangle-vertical'}\n",
      "{'dim': (56, 32, 32), 'images:': 'circle-diagnal'}\n",
      "{'dim': (56, 32, 32), 'images:': 'square-diagnal-2'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'square-vertical-2'}\n",
      "{'dim': (56, 32, 32), 'images:': 'trangle-vertical-3'}\n",
      "{'dim': (56, 32, 32), 'images:': 'trangle-horizontal'}\n",
      "{'dim': (56, 32, 32), 'images:': 'square-vertical-3'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'trangle-vertical-2'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (14, 32, 32), 'images:': 'moving box uniform'}\n",
      "{'dim': (56, 32, 32), 'images:': 'circle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'circle-diagnal-2'}\n",
      "{'dim': (56, 32, 32), 'images:': 'square-vertical'}\n",
      "{'dim': (56, 32, 32), 'images:': 'square-diagnal'}\n",
      "\n",
      "After Augmentation: img_collections has 21 collections, 1128 images in total\n"
     ]
    }
   ],
   "source": [
    "img_collections = get_processed_moving_box()\n",
    "# number of images for each collection\n",
    "num_per_collection = [x.shape[0] for x in img_collections]\n",
    "# number of collections\n",
    "n_collection = len(img_collections)\n",
    "# total number of images\n",
    "total_imgs = sum(num_per_collection)\n",
    "print(\"\\nAfter Augmentation: img_collections has {} collections, {} images in total\".format(n_collection, total_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ind_collection, val_ind_collection = zip(*[split_train_dev(x) for x in img_collections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(batch_size = 8, train=True, gap = 1):\n",
    "    # get average number of training for each class\n",
    "    avg_num_per_class = int(np.ceil(batch_size/n_collection))\n",
    "    # before-index for each class\n",
    "    before_ind = []\n",
    "    for i in range(n_collection):\n",
    "        data = train_ind_collection[i] if train else val_ind_collection[i]\n",
    "        try:\n",
    "            s = np.random.choice(list(filter(lambda x:x<num_per_collection[i]-gap-1, data)),avg_num_per_class, replace=False)\n",
    "            before_ind.append(s)\n",
    "        except:\n",
    "            before_ind.append(np.array([]))\n",
    "    # after-index for each class\n",
    "    after_ind = [x+gap+1 for x in before_ind]\n",
    "    # mid-index for each class\n",
    "    mid_ind = [x+(gap+1)//2 for x in before_ind]\n",
    "    \n",
    "    selected_classed = [i for i in range(n_collection) if before_ind[i].shape[0]>0]\n",
    "    before_imgs = np.concatenate([img_collections[i][before_ind[i]] for i in selected_classed], axis = 0)\n",
    "    after_imgs = np.concatenate([img_collections[i][after_ind[i]] for i in selected_classed], axis = 0)\n",
    "    mid_imgs = np.concatenate([img_collections[i][mid_ind[i]] for i in selected_classed], axis = 0)\n",
    "    \n",
    "    clipped = np.random.choice(range(before_imgs.shape[0]), batch_size, replace=False)\n",
    "    before_imgs = before_imgs[clipped]\n",
    "    mid_imgs = mid_imgs[clipped]\n",
    "    after_imgs = after_imgs[clipped]\n",
    "    return (before_imgs, after_imgs), mid_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_train(batch_size = 8, gap = 1):\n",
    "    return sample(batch_size)\n",
    "def sample_dev(batch_size = 8, gap = 1):\n",
    "    return sample(batch_size, False, gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (4, 32, 32)\n",
      "After:  (4, 32, 32)\n",
      "Mid:    (4, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAACNCAYAAAA5BftnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl/8BeYt89KrMoGYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc16569c6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAACNCAYAAAA5BftnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAJAiOAIAkPJ//ngCo2uUd+gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc163530f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAACNCAYAAAA5BftnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQyG6SrX4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc163530390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(test_before, test_after), test_mid = sample_train(4)\n",
    "print(\"Before: {}\".format(test_before.shape))\n",
    "print(\"After:  {}\".format(test_after.shape))\n",
    "print(\"Mid:    {}\".format(test_mid.shape))\n",
    "size = (12, 2)\n",
    "plot_images(test_before, size, \"Before\")\n",
    "plot_images(test_mid, size, \"Mid\")\n",
    "plot_images(test_after, size, \"After\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(x1, x2,  is_training=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        x1, x2: batch size of images for inference\n",
    "    Output:\n",
    "        predicted images of batch size\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        # reshape and concatenate\n",
    "        x1 = tf.reshape(x1, [-1,  32, 32, 1])\n",
    "        x2 = tf.reshape(x2, [-1,  32, 32, 1])\n",
    "        x = tf.concat([(x1+x2)/2, (x2-x1)], axis=3)\n",
    "        \n",
    "        x = tf.layers.conv2d(x, filters = 128, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "        x = tf.layers.batch_normalization(x, axis=3, training=is_training)\n",
    "        \n",
    "        x = tf.layers.conv2d(x, filters = 64, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "        x = tf.layers.batch_normalization(x, axis=3, training=is_training)\n",
    "        \n",
    "        x = tf.contrib.layers.flatten(x)\n",
    "        x = tf.layers.dense(x, 1024, activation=tf.nn.relu)\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        \n",
    "        x = tf.layers.dense(x, 8*8*128, activation=tf.nn.relu)\n",
    "        x = tf.reshape(x, [-1, 8, 8, 128])\n",
    "        \n",
    "        x = tf.layers.conv2d(x, filters = 32, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "        x = tf.layers.batch_normalization(x, axis=3, training=is_training)\n",
    "        \n",
    "        x = tf.layers.conv2d_transpose(x, filters=64, kernel_size=(4, 4), strides=(2, 2), activation=tf.nn.relu, padding='same')\n",
    "        x = tf.layers.batch_normalization(x,  axis=3, training=is_training)\n",
    "        \n",
    "        ####### split into Mask and Foreground  ####### \n",
    "        # Foreground - relu activation\n",
    "        fg = tf.layers.conv2d_transpose(x, filters=1, kernel_size=(4, 4),  strides=(2, 2), activation=tf.nn.tanh, padding='same')\n",
    "        fg = tf.reshape(fg, [-1, 32, 32, 1])\n",
    "        # Mask\n",
    "        mask = tf.layers.conv2d_transpose(x, filters=64, kernel_size=(4, 4), strides=(2, 2), activation=tf.nn.sigmoid, padding='same')\n",
    "        mask = tf.reshape(mask, [-1, 32, 32, 1])\n",
    "        return fg, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_background(x,  is_training=True):\n",
    "    x = tf.layers.conv2d(x, filters = 32, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, axis=3, training=is_training)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
