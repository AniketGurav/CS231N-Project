{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Stream Generation \n",
    "### Backgroun + Foreground\n",
    "#### Reference: MIT Generating Video with Scene Dynamics[http://web.mit.edu/vondrick/tinyvideo/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from util import *\n",
    "from util.parser import *\n",
    "from util.img_kit import *\n",
    "from util.tf_ops import *\n",
    "from util.notebook_display import *\n",
    "from util.numeric_ops import *\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "from os import walk\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0) # set default size of plots\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': (50, 32, 32), 'images:': 'triangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'trangle-vertical'}\n",
      "{'dim': (56, 32, 32), 'images:': 'circle-diagnal'}\n",
      "{'dim': (56, 32, 32), 'images:': 'square-diagnal-2'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'square-vertical-2'}\n",
      "{'dim': (56, 32, 32), 'images:': 'trangle-vertical-3'}\n",
      "{'dim': (56, 32, 32), 'images:': 'trangle-horizontal'}\n",
      "{'dim': (56, 32, 32), 'images:': 'square-vertical-3'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'trangle-vertical-2'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (14, 32, 32), 'images:': 'moving box uniform'}\n",
      "{'dim': (56, 32, 32), 'images:': 'circle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'rectangle'}\n",
      "{'dim': (56, 32, 32), 'images:': 'circle-diagnal-2'}\n",
      "{'dim': (56, 32, 32), 'images:': 'square-vertical'}\n",
      "{'dim': (56, 32, 32), 'images:': 'square-diagnal'}\n",
      "\n",
      "After Augmentation: img_collections has 21 collections, 1128 images in total\n"
     ]
    }
   ],
   "source": [
    "img_collections = get_processed_moving_box()\n",
    "# number of images for each collection\n",
    "num_per_collection = [x.shape[0] for x in img_collections]\n",
    "# number of collections\n",
    "n_collection = len(img_collections)\n",
    "# total number of images\n",
    "total_imgs = sum(num_per_collection)\n",
    "print(\"\\nAfter Augmentation: img_collections has {} collections, {} images in total\".format(n_collection, total_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ind_collection, val_ind_collection = zip(*[split_train_dev(x) for x in img_collections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(batch_size = 8, train=True, gap = 1):\n",
    "    # get average number of training for each class\n",
    "    avg_num_per_class = int(np.ceil(batch_size/n_collection))\n",
    "    # before-index for each class\n",
    "    before_ind = []\n",
    "    for i in range(n_collection):\n",
    "        data = train_ind_collection[i] if train else val_ind_collection[i]\n",
    "        try:\n",
    "            s = np.random.choice(list(filter(lambda x:x<num_per_collection[i]-gap-1, data)),avg_num_per_class, replace=False)\n",
    "            before_ind.append(s)\n",
    "        except:\n",
    "            before_ind.append(np.array([]))\n",
    "    # after-index for each class\n",
    "    after_ind = [x+gap+1 for x in before_ind]\n",
    "    # mid-index for each class\n",
    "    mid_ind = [x+(gap+1)//2 for x in before_ind]\n",
    "    \n",
    "    selected_classed = [i for i in range(n_collection) if before_ind[i].shape[0]>0]\n",
    "    before_imgs = np.concatenate([img_collections[i][before_ind[i]] for i in selected_classed], axis = 0)\n",
    "    after_imgs = np.concatenate([img_collections[i][after_ind[i]] for i in selected_classed], axis = 0)\n",
    "    mid_imgs = np.concatenate([img_collections[i][mid_ind[i]] for i in selected_classed], axis = 0)\n",
    "    \n",
    "    clipped = np.random.choice(range(before_imgs.shape[0]), batch_size, replace=False)\n",
    "    before_imgs = before_imgs[clipped]\n",
    "    mid_imgs = mid_imgs[clipped]\n",
    "    after_imgs = after_imgs[clipped]\n",
    "    return (before_imgs, after_imgs), mid_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_train(batch_size = 8, gap = 1):\n",
    "    return sample(batch_size)\n",
    "def sample_dev(batch_size = 8, gap = 1):\n",
    "    return sample(batch_size, False, gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (4, 32, 32)\n",
      "After:  (4, 32, 32)\n",
      "Mid:    (4, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAACNCAYAAAA5BftnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBEcAALoIjgAAdBEcAQDo8jsE64R2K/S7DgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8f7ad5f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAACNCAYAAAA5BftnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEgRHAEASBEcAQBI+R9CYAp7Obs/vwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8f7ad5eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAACNCAYAAAA5BftnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqgiOAABU+S8ae0dugjOb+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8f59a8a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(test_before, test_after), test_mid = sample_train(4)\n",
    "print(\"Before: {}\".format(test_before.shape))\n",
    "print(\"After:  {}\".format(test_after.shape))\n",
    "print(\"Mid:    {}\".format(test_mid.shape))\n",
    "size = (12, 2)\n",
    "plot_images(test_before, size, \"Before\")\n",
    "plot_images(test_mid, size, \"Mid\")\n",
    "plot_images(test_after, size, \"After\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_loss(x, gd):\n",
    "    x, gd = tf.contrib.layers.flatten(x), tf.contrib.layers.flatten(gd)\n",
    "    return tf.norm(x-gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_background(x, is_training=True):\n",
    "    x = tf.layers.conv2d(x, filters = 32, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, axis=3, training=is_training)\n",
    "    \n",
    "    x = tf.contrib.layers.flatten(x)\n",
    "    x = tf.layers.dense(x, 1024, activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=is_training)\n",
    "    \n",
    "    x = tf.reshape(x, [-1, 32, 32, 1])\n",
    "    x = tf.layers.conv2d(x, filters = 32, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, axis=3, training=is_training)\n",
    "    \n",
    "    x = tf.layers.conv2d_transpose(x, filters=64, kernel_size=(4, 4), strides=(2, 2), activation=tf.nn.tanh, padding='same')\n",
    "    x = tf.reshape(x, [-1, 32, 32, 1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(x1, x2,  is_training=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        x1, x2: batch size of images for inference\n",
    "    Output:\n",
    "        predicted images of batch size\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        # reshape and concatenate\n",
    "        x1 = tf.reshape(x1, [-1,  32, 32, 1])\n",
    "        x2 = tf.reshape(x2, [-1,  32, 32, 1])\n",
    "        x = tf.concat([(x1+x2)/2, (x2-x1)], axis=3)\n",
    "        \n",
    "        background= extract_background(x, is_training)\n",
    "        print(tf.shape(background))\n",
    "        \n",
    "        x = tf.layers.conv2d(x, filters = 128, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "        x = tf.layers.batch_normalization(x, axis=3, training=is_training)\n",
    "        \n",
    "        x = tf.layers.conv2d(x, filters = 64, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "        x = tf.layers.batch_normalization(x, axis=3, training=is_training)\n",
    "        \n",
    "        x = tf.contrib.layers.flatten(x)\n",
    "        x = tf.layers.dense(x, 1024, activation=tf.nn.relu)\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        \n",
    "        x = tf.layers.dense(x, 8*8*128, activation=tf.nn.relu)\n",
    "        x = tf.reshape(x, [-1, 8, 8, 128])\n",
    "        \n",
    "        x = tf.layers.conv2d(x, filters = 32, kernel_size=2, padding='same', activation=tf.nn.relu)\n",
    "        x = tf.layers.batch_normalization(x, axis=3, training=is_training)\n",
    "        \n",
    "        x = tf.layers.conv2d_transpose(x, filters=64, kernel_size=(4, 4), strides=(2, 2), activation=tf.nn.relu, padding='same')\n",
    "        x = tf.layers.batch_normalization(x,  axis=3, training=is_training)\n",
    "        \n",
    "        ####### split into Mask and Foreground  ####### \n",
    "        # Foreground - relu activation\n",
    "        fg = tf.layers.conv2d_transpose(x, filters=1, kernel_size=(4, 4),  strides=(2, 2), activation=tf.nn.tanh, padding='same')\n",
    "        fg = tf.reshape(fg, [-1, 32, 32, 1])\n",
    "        # Mask\n",
    "        mask = tf.layers.conv2d_transpose(x, filters=64, kernel_size=(4, 4), strides=(2, 2), activation=tf.nn.sigmoid, padding='same')\n",
    "        mask = tf.reshape(mask, [-1, 32, 32, 1])\n",
    "        \n",
    "        # Assemble background, foreground, and mask into final image\n",
    "        print(mask.get_shape())\n",
    "        img = mask * fg + (1-mask) * background\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_generattion(before, after, mid):\n",
    "    dic = {batch_before: before, batch_after: after, batch_mid: mid, is_training: False}\n",
    "    gen_batch = sess.run(G_batch, dic)\n",
    "    gen_batch, loss = sess.run([G_batch, G_loss], dic)\n",
    "    return gen_batch, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gap = 1\n",
    "batch_size = 16\n",
    "\n",
    "learning_rate = 2e-4\n",
    "beta = 0.9\n",
    "num_iteration = 4000\n",
    "relu_alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"generator/Shape:0\", shape=(4,), dtype=int32)\n",
      "(?, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_mid        = tf.placeholder(tf.float32, [None, 32, 32])\n",
    "batch_before     = tf.placeholder(tf.float32, [None, 32, 32])\n",
    "batch_after      = tf.placeholder(tf.float32, [None, 32, 32])\n",
    "is_training      = tf.placeholder(tf.bool, ())\n",
    "\n",
    "G_batch = generate(batch_before, batch_after)\n",
    "\n",
    "G_loss = content_loss(G_batch, batch_mid)\n",
    "_, G_solver = get_solvers_adam(learning_rate, beta)\n",
    "\n",
    "G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'generator') \n",
    "\n",
    "G_train_step = G_solver.minimize(G_loss, var_list=G_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, G_step, G_loss, batch_size, num_iteration, plot_every = 400, show_loss_every=400, nPlot=6):\n",
    "    g_losses = []\n",
    "    for i in range(num_iteration):\n",
    "        (real_before, real_after), real_mid = sample_train(batch_size, gap)\n",
    "        dic = {batch_mid: real_mid, batch_before: real_before, batch_after: real_after, is_training: True}\n",
    "        _, G_loss_curr = sess.run([G_train_step, G_loss], dic)\n",
    "        print(batch_mid)\n",
    "        g_losses.append(G_loss_curr)\n",
    "        if i%show_loss_every ==0:\n",
    "            print(\"Iteration {}:  G_loss = {}\".format(i, G_loss_curr))\n",
    "            \n",
    "        if i%plot_every == 0:\n",
    "            gen_batch_test = sess.run(G_batch, feed_dict=\\\n",
    "                                      {batch_before: real_before, batch_after: real_after, is_training: False})\n",
    "            plot_batch_images(gen_batch_test[:nPlot], (16, 2) , \"Iteration: {}\".format(i))\n",
    "    return g_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [1024,32,32,1] vs. [16,32,32,1]\n\t [[Node: generator/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](generator/Reshape_6, generator/Reshape_5)]]\n\t [[Node: norm/Squeeze/_39 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2757_norm/Squeeze\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'generator/mul', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 405, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-48-f492782523a9>\", line 7, in <module>\n    G_batch = generate(batch_before, batch_after)\n  File \"<ipython-input-45-ab3ddbe0ee49>\", line 46, in generate\n    img = mask * fg + (1-mask) * background\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 794, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1015, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1625, in _mul\n    result = _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [1024,32,32,1] vs. [16,32,32,1]\n\t [[Node: generator/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](generator/Reshape_6, generator/Reshape_5)]]\n\t [[Node: norm/Squeeze/_39 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2757_norm/Squeeze\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [1024,32,32,1] vs. [16,32,32,1]\n\t [[Node: generator/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](generator/Reshape_6, generator/Reshape_5)]]\n\t [[Node: norm/Squeeze/_39 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2757_norm/Squeeze\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-2d2528ff95e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG_train_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-49-96b2bb0b8526>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(sess, G_step, G_loss, batch_size, num_iteration, plot_every, show_loss_every, nPlot)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mreal_before\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_after\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mdic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mbatch_mid\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreal_mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_before\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreal_before\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_after\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreal_after\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG_loss_curr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mG_train_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mg_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG_loss_curr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1035\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [1024,32,32,1] vs. [16,32,32,1]\n\t [[Node: generator/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](generator/Reshape_6, generator/Reshape_5)]]\n\t [[Node: norm/Squeeze/_39 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2757_norm/Squeeze\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'generator/mul', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 405, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-48-f492782523a9>\", line 7, in <module>\n    G_batch = generate(batch_before, batch_after)\n  File \"<ipython-input-45-ab3ddbe0ee49>\", line 46, in generate\n    img = mask * fg + (1-mask) * background\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 794, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1015, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1625, in _mul\n    result = _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [1024,32,32,1] vs. [16,32,32,1]\n\t [[Node: generator/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](generator/Reshape_6, generator/Reshape_5)]]\n\t [[Node: norm/Squeeze/_39 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2757_norm/Squeeze\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "sess = get_session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "g_loss = train(sess, G_train_step, G_loss, batch_size, num_iteration, plot_every = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figsize = (8, 2)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.plot(g_loss)\n",
    "plt.title(\"Generator Losses\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
